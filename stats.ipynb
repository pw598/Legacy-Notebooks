{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af039cc-7b5f-4465-8a55-1d8e39ce4409",
   "metadata": {},
   "source": [
    "# Essential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8e907-a8f5-4829-a278-2fbd8e5a9a28",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62079336-dd33-4b5b-9dbe-08405f72169d",
   "metadata": {},
   "source": [
    "Summarizing data is important to understanding it at scale, and descriptive statistics help us to do so.\n",
    "\n",
    "----\n",
    "<i>\"Statistics is a broad set of algorithms for transforming numerical data into a small set of interpretable values that describe the world.\"</i>\n",
    "- Modern Statistics, Mike X. Cohen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f411ec-ea31-4dd7-ba23-0c0390fade8a",
   "metadata": {},
   "source": [
    "#### Mean, Median, and Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e9a34-5aef-45d9-9aa3-c8730529d701",
   "metadata": {},
   "source": [
    "The mean of a series of numbers is the average, the sum divided by the count.\n",
    "\n",
    "<h4>$\\bar{x} ~~=~~ \\frac{ \\sum_{i=1}^n x_i }{n} ~~= ~~\\frac{1}{n} \\sum_{i=1}^n x_i ~~=~~ n^{-1} \\sum_{i=1}^n x_i$</h4>\n",
    "\n",
    "- $\\bar{x}$ is the mean of the series of numbers $x$\n",
    "- $\\sum_{i=1}^n$ indicates a summation of all $x_i$ in $x$\n",
    "- $n$ is the number of observations\n",
    "- $i$ is the index of a particular data point of $x$\n",
    "- $x_i$ is a data point of $x$ with index $i$\n",
    "- $1/n$ is a multiplication equivalent to dividing by $n$\n",
    "- $n^{-1}$ is equal to $1/n$\n",
    "\n",
    "The median is the data point in the middle of the list when you sort the series of numbers $x$ by value. It can be preferable to the mean as a measure of central tendency, as it is unaffected by outliers.\n",
    "\n",
    "The mode is the value observed most frequently in the series of numbers, and can be tied amongst two or more values. With an unskewed Gaussian/normal distribution, the median and mode happen to be equal to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc98df-3467-4e6e-b9b5-2e3966504b2a",
   "metadata": {},
   "source": [
    "#### Percentile and Quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3aabe-7b2b-4298-a843-c2f7baf0eac6",
   "metadata": {},
   "source": [
    "You can think of the concept of percentiles as a generalization of the concept of the median, as the median represents the value at the $50^{th}$ percentile. Other special cases of the percentile are the values $25\\%$ and $75\\%$ of the way down the list of sorted values, and together with the median, these are called quartiles, because they divide the data into four sections with an equal number of observations. The range between the $25^{th}$ and $75^{th}$ quartile is called the interquartile range (IQR). \n",
    "\n",
    "The percentiles at multiples of $10\\%$ are called deciles, and multiples of $1\\%$ are called quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795f447-66ce-42fd-b1e7-d5447b3e26ce",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd82c29a-7310-42e0-914c-7d589a3659e6",
   "metadata": {},
   "source": [
    "Variance is a measure of how much spread there is around the expected value, and is calculated as the average of squared differences from the expected value. Squared because we don't want negative differences to offset the positive ones, but rather blend together to give us an idea of average distance regardless of direction. A nuance is that when dealing with a sample rather than a population, we divide by $n-1$ instead of $n$.\n",
    "\n",
    "<i>Population Variance:</i>\n",
    "\n",
    "<h4>$\\sigma^2 = \\frac{\\sum(x - \\mu)^2}{n}$</h4>\n",
    "\n",
    "<i>Sample Variance:</i>\n",
    "\n",
    "<h4>$s^2 = \\frac{\\sum(x - \\bar{x})^2}{n-1}$</h4>\n",
    "\n",
    "- $\\sigma^2$ is the population variance\n",
    "- $s^2$ is the sample variance\n",
    "- $x$ is the value of an instance in the dataset\n",
    "- $\\mu$ is the population average value\n",
    "- $\\bar{x}$ is the sample average value\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817073dd-8a59-420e-b547-b8f59e04c1ce",
   "metadata": {},
   "source": [
    "#### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152552cf-b106-4bfd-aaec-3b20aa4a04db",
   "metadata": {},
   "source": [
    "The squared values of variance make for an interpretability issue - the units of variance are not in the same unit of measurement of the data. This is why we commonly speak in terms of standard deviation - the square root of variance - which brings us back to the same units as the data.\n",
    "\n",
    "<i>Population Standard Deviation:</i>\n",
    "\n",
    "<h4>$\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum(x - \\mu)^2}{n}}$</h4>\n",
    "\n",
    "<i>Sample Standard Deviation:</i>\n",
    "\n",
    "<h4>$s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum(x - \\bar{x})^2}{n-1}}$</h4>\n",
    "\n",
    "- $\\sigma$ is the population standard deviation\n",
    "- $s$ is the sample standard deviation\n",
    "- $x$ is the value of an instance in the dataset\n",
    "- $\\mu$ is the population average value\n",
    "- $\\bar{x}$ is the sample average value\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906fbff-c6de-4487-bb7e-f584114c66e6",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb77e1c-b72f-4fa2-89c9-ad290bcf1b22",
   "metadata": {},
   "source": [
    "You are likely familiar with the concept of correlation - the degree to which the behavior of one variable explains another. To understand correlation, it helps to understand covariance, the unscaled version of correlation. While correlation ranges $-1$ to $+1$, covariance ranges $-X$ to $+X$, and correlation is just scaled covariance.\n",
    "\n",
    "Covariance is the sum of the product of differences from the mean for two variables, scaled by the number of observations $n$, or $n-1$ if sampling. Two coinciding positive differences make for a large contribution to covariance, as do two coinciding negative differences, through multiplication.\n",
    "\n",
    "<i>Population Covariance:</i>\n",
    "\n",
    "<h4>$cov(x,y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n}$</h4>\n",
    "\n",
    "<i>Sample Covariance:</i>\n",
    "\n",
    "<h4>$cov(x,y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n-1}$</h4>\n",
    "\n",
    "- $x$ is a vector (series) of values\n",
    "- $y$ is a vector of values\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\bar{x}$ is the average value of $x$\n",
    "- $y_i$ is an individual data point of $y$\n",
    "- $\\bar{y}$ is the average value of $y$\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea7c94-ca8a-43e7-9315-0d42dce666b9",
   "metadata": {},
   "source": [
    "Back to correlation - correlation is covariance with the numerator scaled by the product of standard deviations for the two variables, rather than $n$ or $n-1$.\n",
    "\n",
    "<h4>$r = \\frac{ \\sum (x_i - \\bar{x}) (y_i - \\bar{y}) }{ \\sqrt{\\sum(x_i - \\bar{x})^2} \\sqrt{\\sum(y_i - \\bar{y})^2} }$</h4>\n",
    "\n",
    "- $x$ is a vector of values\n",
    "- $y$ is a vector of values\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\bar{x}$ is the average value of $x$\n",
    "- $y_i$ is an individual data point of $y$\n",
    "- $\\bar{y}$ is the average value of $y$\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c943d-f43f-4a5c-9a4c-391598283c39",
   "metadata": {},
   "source": [
    "#### Z-Scores and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef24b2-6981-4e79-b4b4-7aad5ad3fb14",
   "metadata": {},
   "source": [
    "Z-Scores indicate how many standard deviations away a data point is from the mean of the dataset.\n",
    "\n",
    "<h4>$z = \\frac{x_i - \\mu}{\\sigma}$</h4>\n",
    "\n",
    "- $z$ is the distance in standard deviations of $x_i$ from the mean of $x$, $\\mu$\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\mu$ is the average value of $x$\n",
    "- $\\sigma$ is the standard deviation of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84721b-dac8-4102-8715-5f81db32aa3c",
   "metadata": {},
   "source": [
    "Z-scores can be helpful for standardization, a form of scaling in which data points are expressed in terms of the number of standard deviations away from the mean.\n",
    "\n",
    "Another way to scale data is with min/max scaling, which follows the following formula:\n",
    "\n",
    "<h4>$scaled ~x_i = \\frac{x_i - min(x)}{max(x) - min(x)}$</h4>\n",
    "\n",
    "- $scaled ~x_i$ is the proportion of range from the minimum to maximum of the vector $x$\n",
    "- $x_i$ is an individual instance of $x$\n",
    "- $min(x)$ is the minimum value of $x$\n",
    "- $max(x)$ is the maximum value of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f21d5-2534-40c6-ba8b-930f5d897d1a",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55106986-a3de-495d-913b-4056b7904905",
   "metadata": {},
   "source": [
    "#### Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf2646-2165-4a2e-a668-1abbde1f83b6",
   "metadata": {},
   "source": [
    "Inferential statistics refers to algorithms that are applied to one or more sample datasets in order to test whether the descriptive statistics are likely to generalize to another dataset.\n",
    "\n",
    "Probability distributions provide the mathematical framework for describing the likelihood of events in a random process or experiment. Methods like maximum likelihood estimation use analytical functions to find parameters that best fit observed data to a theoretical distribution.\n",
    "\n",
    "----\n",
    "<i>It would not be controversial to claim that inferential statistics is basically just applied probability.</i>\n",
    "- Modern Statistics, Mike X. Cohen\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0276ad-acad-4637-af80-2114a9c2f51c",
   "metadata": {},
   "source": [
    "#### Sampling and Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46dcc0-5000-450b-9891-2021f5ed57ca",
   "metadata": {},
   "source": [
    "Sample size is the number of observations in a dataset. It is often denoted by N, but also by n. We can generalize about the population from a sample only if the sample is random, representative, and sufficiently large. An appropriate sample size depends on a number of factors, including effect size, variability in the sample and population, how closely the sample characteristics match the population, and how the samples were collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684aa444-3def-4582-8715-f1376c476e3d",
   "metadata": {},
   "source": [
    "#### The Gaussian (Normal) and t-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72826124-280c-4a59-b3ec-3139f8d27eb6",
   "metadata": {},
   "source": [
    "Going forward, we will assume either a Gaussian/normal distribution, reflecting an assumption of known population parameters, or the t-distribution, reflecting smaller sample sizes. The Gaussian distribution represents many natural phenomena, and many other distributions (including the t-distribution) converge to it when mixed with others, or when sample size is high.\n",
    "\n",
    "The parameters of the Gaussian distribution are mean $\\mu$ and variance $\\sigma^2$, and the parameter of the t-distribution is the degrees of freedom, symbolized by $\\nu$ (pronounced nu), which for one-sample and paired tests, is set to $n-1$ (where n is the number of observations). We'll talk more about the t-distribution and degrees of freedom when we get to t-tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f5ee7-75bf-4592-bbb5-a1f8a5c14834",
   "metadata": {},
   "source": [
    "#### p-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dff1e8-4e43-4fc0-a44f-ea2573e44736",
   "metadata": {},
   "source": [
    "If testing the effect of an experiment, we expect to see that a sample mean from the experiment is unlikely to come from the assumed probability distribution, meaning that the difference in treatment/environment/etc. has had a significant effect.\n",
    "\n",
    "A p-value (probability value) is the probability than an effect you observe in data was actually due to chance and not a true effect. A significance level $\\alpha$, reflecting the maximum threshold for the p-value, is commonly selected to be 0.05 or 0.01. This reflects the chance of making a 'type 1 error', a.k.a. a false positive, where a test incorrectly indicates that a condition is present when it is not. It is when the p-value is lower than the significance level that we consider a result to be significant, i.e., not likely due to random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59fb5c-a9cb-4a5b-b584-eb421447d9ea",
   "metadata": {},
   "source": [
    "#### Null and Alternative Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b018e-b215-46ed-ae21-3e67cfdbfb2e",
   "metadata": {},
   "source": [
    "The null hypothesis $H_0$ is that there is no difference between the sampled data and the population. The drug had no effect, the sale provided no uplift, etc. When the p-value is less than the significance level $\\alpha$, the null hypothesis $H_0$ is rejected in favor of the alternative hypothesis $H_A$ (or $H_1$). Rejecting the null hypothesis does not prove that the alternative hypothesis is true, but suggests there is sufficient enough evidence that the null hypothesis is unlikely to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75563c56-d6a0-42e6-8960-c94bae0a0be6",
   "metadata": {},
   "source": [
    "#### Critical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b942-2de2-452a-886e-88c68f9246d7",
   "metadata": {},
   "source": [
    "A critical value is defined in the context of the population distribution and a probability, and is used as a threshold for interpreting the result of a statistical test. The values in the population beyond the critical value are called the critical region or region of rejection.\n",
    "\n",
    "A one-tailed test has a single critical value, on the left or the right of the distribution, and if the calculated statistic is less or equally extreme than the critical value, the null hypothesis of the test fails to be rejected. A two-tailed test has two critical values, one on each side of the distribution, which is often assumed to be symmetrical. When using a two-tailed test, the significance level $\\alpha$ used in the calculation of critical values must be divided by two.\n",
    "\n",
    "<img src=\"img/critical_region.png\" style=\"height: 400px; width:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fff88b-ecff-4d74-925b-359e3201f6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f023087-e887-4860-a660-d976af966d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content re: reference tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3d188-39a3-4dbe-9694-516950a70b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2053dab6-7fd5-429e-9879-434c79961b93",
   "metadata": {},
   "source": [
    "### The Z-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13759558-ac73-4d2c-baae-456efc638777",
   "metadata": {},
   "source": [
    "A z-test quantifies the probability of a number as or more extreme than another given number, when drawn from a normal distribution, given the data observed. Notable p-z combinations include:\n",
    "- 68.3% of data is between -1 and +1 standard deviations from the mean\n",
    "- 95.5% of data is between -2 and +2 standard deviations from the mean\n",
    "- 99.7% of data is between -3 and +3 standard deviations from the mean\n",
    "\n",
    "<img src=\"img/norm_dist.png\" style=\"height: 350px; width:auto;\">\n",
    "\n",
    "A z-test for a population mean investigates the significance of the difference between an assumed population mean $\\mu_0$ and a sample mean $\\bar{x}$. For  test, we assume to known the population variance $\\sigma^2$. If we don't assume to know the variance, then the t-test for a population mean should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae5a01-aa5d-4e1f-ba50-66e46ec31d81",
   "metadata": {},
   "source": [
    "#### The Z-Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe72cfb-851a-488f-8fb8-f8712a80a588",
   "metadata": {},
   "source": [
    "The z-statistic, like the t-statistic, is equal to the mean of a sample divided by the standard error. The standard error of the mean is a descriptive statistic that you can compute from a sample, and estimates the precision with which that sample mean estimates the population mean. It is defined as the population standard deviation scaled by the square root of the sample size.\n",
    "\n",
    "$SE = \\sigma / \\sqrt{n}$\n",
    "\n",
    "- $SE$ is the standard error of the mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "- $n$ is the number of observations\n",
    "\n",
    "For a sample, we would use $s$, the sample standard deviation, instead of \\sigma. \n",
    "\n",
    "Back to the z-statistic: from a population with assumed mean $\\mu_0$ and known variance $\\sigma^2$, a random sample of size $n$ is taken and the sample mean $\\bar{x}$ calculated. The test statistic,\n",
    "\n",
    "<h4>$z = \\frac{\\bar{x} - \\mu_o}{\\sigma / \\sqrt{n}}$</h4>\n",
    "\n",
    "- $z$ is the calculated test statistic\n",
    "- $\\bar{x}$ is the average value of variable $x$\n",
    "- $\\mu_0$ is the null hypothesis mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "- $n$ is the number of observations\n",
    "\n",
    "may be compared with the standard normal distribution using either a one-tailed or two-tailed test (with critical region of size $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c08b6-3d30-45c9-9753-c4cadb2bbe21",
   "metadata": {},
   "source": [
    "#### Example: Z-Test for Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ef51d-61e8-4dd9-8f8e-f51ba486146b",
   "metadata": {},
   "source": [
    "A cosmetics filling process fills tubs of powder with $4$ grams on average with a standard deviation of $1$ gram. A sample of $9$ tubs are weighed, and the average weight is $4.6$ grams. What can be said about the filling process, at a significance level of $0.05$?\n",
    "\n",
    "- $\\bar{x} = 4.6$\n",
    "- $\\mu_0 = 4.0$\n",
    "- $\\sigma = 1.0$\n",
    "- $n = 9$\n",
    "\n",
    "<h4>$z = \\frac{\\bar{x} - \\mu_o}{\\sigma / \\sqrt{n}} = \\frac{4.6-4.0}{1 / \\sqrt{9}} = 1.8$</h4>\n",
    "\n",
    "The critical value $z_{0.05} = 1.96$. The z-stat is not as extreme as the critical z; our range of acceptance of the null hypothesis is $-1.96$ to $1.96$, and so we fail to reject the null hypothesis.\n",
    "\n",
    "However, if we are only concerned about over-filling and not under-filling, it becomes a one-tailed test instead of a two-tailed test, in which the acceptance region is now $z \\lt 1.645$. Therefore, we can reject the null hypothesis and suspect that the tubs are being over-filled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d4df8-b6e6-4f17-a8cd-e23b64601777",
   "metadata": {},
   "source": [
    "### The T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa4e66-6ad2-411e-86e2-bbdf8e724ccd",
   "metadata": {},
   "source": [
    "Like the z-test for larger samples, the objective of a t-test can be to investigate the significance of the difference between an assumed mean $\\mu_0$ and unknown variance. A random sample of size $n$ is taken and the sample mean $\\bar{x}$ calculated as well as the sample standard deviation. The test statistic, \n",
    "\n",
    "The test statistic, \n",
    "\n",
    "<h4>$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$</h4>\n",
    "\n",
    "- $t$ is the calculated test statistic\n",
    "- $\\bar{x}$ is the average value of variable $x$\n",
    "- $\\mu_0$ is the null hypothesis mean\n",
    "- $s$ is the standard deviation\n",
    "- $n$ is the number of observations\n",
    "\n",
    "may be compared with the t-distribution (a.k.a. Student's t-distribution, as the originator used the pseudonym 'Student'), with $n-1$ degrees of freedom, and may be one-tailed or two-tailed. The t-distribution approximates a normal distribution as the number of samples increases, but for smaller samples, is shorter and more spread out.\n",
    "\n",
    "<img src=\"img/t_dist.png\" style=\"height: 350px; width:auto;\">\n",
    "\n",
    "The purpose of a t-test is to determine whether the mean of a sample is different from a specified null hypothesis $H_0$ value. There are three scenarios in which you would use a t-test:\n",
    "\n",
    "1. One-Sample T-Test: you have one data sample, and the objective is to determine if the sample mean significantly deviates from a predetermined $H_0$ value.\n",
    "\n",
    "2. Paired Samples T-Test: you have one group of individuals that were measured twice; for example, before and after a treatment.\n",
    "\n",
    "3. Independent Samples T-Test: you have two separate groups of individuals and want to determine whether the means of the two groups differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629886ef-47bf-473a-bc5a-64829805e7f0",
   "metadata": {},
   "source": [
    "#### Degrees of Freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60d0aa-b2d3-4ee8-a17e-02cdaaaba4b4",
   "metadata": {},
   "source": [
    "If I tell you that the average of three numbers is 4, and that two of those numbers are 2 and 3, then you know that the third number is 7. In other words, the statistic has $n-1=2$ numbers that can vary before the last number is fixed. We say that there are 2 degrees of freedom.\n",
    "\n",
    "Using $n-1$ as the degree of freedom for one parameter (variable) generalizes to using $n-k$ in the multivariate case, where $n$ is the number of observations and $k$ is the number of parameters.\n",
    "\n",
    "The degrees of freedom associated with a t-test is $n-1$ or $n-2$ depending on whether there is one group or two. The degrees of freedom associated with a correlation analysis is 2, and the degrees of freedom associated with a regression is $n-k$, where $k$ is the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7648f-a1b8-42ac-ac48-c31733595c71",
   "metadata": {},
   "source": [
    "#### T-Test for a Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d09ece-a3f8-48c5-a904-86cfeb9de92d",
   "metadata": {},
   "source": [
    "#### Example: T-Test for a Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8746c6-b899-4e8b-9076-12f8ac1cbc01",
   "metadata": {},
   "source": [
    "A sample of $9$ plastic nuts yielded an average diameter of $3.1cm$ and estimated standard deviation of $1.0cm$. The population mean is assumed to be $4.0cm$. What does this say about the mean diameter of plastic nuts being produced?\n",
    "\n",
    "- $\\bar{x} = 4.27$\n",
    "- $\\mu_0 = 4.0$\n",
    "- $s = 0.27$\n",
    "- $n = 9$\n",
    "\n",
    "<h4>$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} = \\frac{4.27 - 4.0}{0.27 / \\sqrt{9}} = 2.92$</h4>\n",
    "\n",
    "Our computed t value is $2.92$ and the critical value is $t_{8; 0.025} = \\pm 2.3$. The acceptance region is $-2.3$ to $2.3$, so we reject the null hypothesis and accept the alternative hypothesis that there is a difference between the sample and population means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a529c3-aaca-4593-98b9-af73f90594bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31368385-fcc8-47ae-b2ce-46b37555eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adjusted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39bf5c-2ed4-4525-981d-94d7f542a0b4",
   "metadata": {},
   "source": [
    "- $\\bar{x} = 4.27$\n",
    "- $\\mu_0 = 4.0$\n",
    "- $s = 0.27$\n",
    "- $n = 9$\n",
    "\n",
    "<h4>$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} = \\frac{3.1 - 4.0}{1.0 / \\sqrt{9}} = -2.7$</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8291b-053a-427a-ac76-5df7ca29ba45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb70920-a365-49ea-9721-c63ce23ec510",
   "metadata": {},
   "source": [
    "#### Example 2: T-Test for a Population Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffa1e3-cb66-4f6e-bae2-01331ead3724",
   "metadata": {},
   "source": [
    "A teacher wants to know if the average exam score of her students is significantly different from the national average of $75$ points. There are $15$ students, and the grades are as follows:\n",
    "\n",
    "$[80, 85, 90, 70, 75, 72, 88, 77, 82, 65, 79, 81, 74, 86, 68]$\n",
    "\n",
    "$H_0: \\bar{X} = 75$\n",
    "\n",
    "$H_A: \\bar{X} \\neq 75$\n",
    "\n",
    "Mean $\\bar{X} = 78.1$\n",
    "\n",
    "$s = 7.5$\n",
    "\n",
    "The test statistic is:\n",
    "\n",
    "$t_{14} = \\frac{78.1-75}{7.5/\\sqrt{15}} = 1.624$\n",
    "\n",
    "$t_{14} = 1.624, p \\lt 0.127$\n",
    "\n",
    "Because the p-value is larger than $0.05$, we cannot reject the $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766b315-f15c-49ae-a4de-7a6b747d1141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ac3acea-85d5-4b48-bbfd-f27e6f68bfd2",
   "metadata": {},
   "source": [
    "#### T-Test for Two Population Means - Method of Paired Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d84c88-2aaa-4783-854c-c62efb69be7f",
   "metadata": {},
   "source": [
    "This test is to investigate the significance of the difference between two population means, $\\mu_1$ and $\\mu_2$, making no assumption about the population variances. The observations for the two samples must be obtained in pairs. Apart from population differences, the observations in each pair should be carried out under identical or near-identical conditions.\n",
    "\n",
    "The differences $d_i$ are formed for each pair of observations. If there are $n$ such pairs of observations, we can calculate the variance of the differences by:\n",
    "\n",
    "<h4>$s^2 = \\sum_{i=1}^n \\frac{ (d_i - \\bar{d})^2 }{n-1}$</h4>\n",
    "\n",
    "- $s^2$ is the variance of the differences\n",
    "- $\\sum_{i=1}^n$ is a summation over the differences for each individual\n",
    "- $d_i$ is the difference (such as before vs. after) for the individual indexed by $i$\n",
    "- $\\bar{d}$ is the average differencel for all individuals\n",
    "\n",
    "The test statistic becomes:\n",
    "\n",
    "<h4>$t = \\frac{ \\bar{x}_1 - \\bar{x}_2 }{s / \\sqrt{n}} = \\frac{\\bar{d}-0}{s/\\sqrt{n}}$</h4> \n",
    "\n",
    "- $t$ is the calculated test statistic\n",
    "- $\\bar{x}_1$ is the mean of the first group of observations\n",
    "- $\\bar{x}_2$ is the mean of the second group of observations\n",
    "- $s$ is the standard deviation, the square root of the variance calculation above\n",
    "- $n$ is the number of observations\n",
    "- $\\bar{d}$ is the mean difference between the two groups of observations\n",
    "\n",
    "which follows the Student's t-distribution with $n-1$ degrees of freedom. The test may be one-tailed or two-tailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892b057-32d4-4201-a56c-4bfb117681ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3fbc6d-0b1e-4ddf-bfcb-eae4a4309fcc",
   "metadata": {},
   "source": [
    "#### Example: Method of Paired Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ec428-4747-40f7-a668-e3dc24c4dbfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "To compare the efficacy of two treatments for a respiratory condition, $10$ patients are randomly selected and the treatments are administered using a spray. Time is taken to ensure the treatments don't interact. We do not expect one particular treatment to be superior to the other so it is a two-tailed test.\n",
    "\n",
    "data...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $\\bar{d} = \\bar{x}_1 - \\bar{x}_2 = -0.1$\n",
    "- $s = 2.9$\n",
    "- $n = 10$\n",
    "- $\\nu = 9$\n",
    "\n",
    "<h4>$t = \\frac{ \\bar{x}_1 - \\bar{x}_2 }{s / \\sqrt{n}} = \\frac{\\bar{d}-0}{s/\\sqrt{n}} = \\frac{-0.1}{2.9 / \\sqrt{10}} = -0.11$</h4>\n",
    "\n",
    "The critical t-statistic is $t_{9; 0.025} = 2.26$. We do not reject the null hypothesis of no difference between means.\n",
    "\n",
    "However, if the objective was to see whether one treatment is superior to another, a one-tailed t-test may be used..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c0b9f-0c27-4e26-9041-faa584b7d49b",
   "metadata": {},
   "source": [
    "#### Example 2: Method of Paired Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5663aa9-3ebd-4df3-a571-397ce2625c93",
   "metadata": {},
   "source": [
    "The effect of a treatment for milk production in dairy cows is tested. The milk yields were measured before and after.\n",
    "\n",
    "Before = [27, 45, 38, 20, 22, 50, 40, 33, 18]\n",
    "\n",
    "After = [31, 54, 43, 28, 21, 49, 41, 34, 20]\n",
    "\n",
    "Diffs = [4, 9, 5, 8, -1, -1, 1, 1, 2]\n",
    "\n",
    "$s = \\sqrt{ \\frac{\\sum_i (y_i - \\mu)}{n-1} } = 3.655$\n",
    "\n",
    "$t = \\frac{\\bar{d} - 0}{s / \\sqrt{n}} = \\frac{3.11 - 0}{3.655 / \\sqrt{9}} = 2.553$\n",
    "\n",
    "**reframe calc to match equations in both problems**\n",
    "\n",
    "The critical value for $n-1=8$ d.f. is $t_{0.05} = 2.306$\n",
    "\n",
    "The calculated value $t=2.553$ is more extreme than the critical value $2.306$, the null hypothesis is rejected with $\\alpha=0.05$ level of significance (the treatment increases milk yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff2fb6-6385-4f3d-ae3c-62ad402efaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9af7f66-8822-43e5-b81f-cba02cbea456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adjusted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a33c8-37c2-40bb-acb9-5c967f38c402",
   "metadata": {},
   "source": [
    "- $\\bar{x}_1 = 32.56$\n",
    "- $\\bar{x}_2 = 35.67$\n",
    "- $\\bar{d} = 3.11$\n",
    "- $n = 9$\n",
    "\n",
    "$s = \\sqrt{ \\frac{\\sum_i (d_i - \\bar{d})^2}{n-1} } = 3.655$\n",
    "\n",
    "$t = \\frac{\\bar{d} - 0}{s / \\sqrt{n}} = \\frac{3.11 - 0}{3.655 / \\sqrt{9}} = 2.553$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307d526-fa44-402a-b80c-90fb4f17fd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f65d676-1be5-4789-b663-9f845c7c4493",
   "metadata": {},
   "source": [
    "#### Independent Samples T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c19f5-48df-4470-b017-a3ee64960904",
   "metadata": {},
   "source": [
    "The independent samples t-test evaluates whether the means of two groups significantly differ. The sample sizes may differ between the two groups. The formula that separates the sample sizes and variances is called Welch's t-test.\n",
    "\n",
    "<h3>$t = \\frac{ \\bar{x} - \\bar{y} }{ \\sqrt{ \\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y} } }$</h3>\n",
    "\n",
    "- $t$ is the calculated t-statistic\n",
    "- $\\bar{x}$ is the average of variable $x$\n",
    "- $\\bar{y}$ is the average of variable $y$\n",
    "- $s_x$ is the sample standard deviation for $x$\n",
    "- $s_y$ is the sample standard deviation for $y$\n",
    "- $n_x$ is the number of observations in variable $x$\n",
    "- $n_y$ is the number of observations in variable $y$\n",
    "\n",
    "If the variances are roughly equal, the degrees of freedom are $n_x + n_y - 2$. If unequal, the formula is a little complicated:\n",
    "\n",
    "(**move this to appendix**)\n",
    "\n",
    "<h3>$\\text{d.f.} = \\frac{ (s_1^2~/~n_1 +s_2^2~/~n_2)^2 }{ \\frac{s_1^2}{n_1^2(n-1)} + \\frac{s^2}{n_1^2(n_1-1)} }$</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467837c-8a01-4612-9afd-a3ba75933fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9532d0bb-2787-4718-829b-0614563e15ca",
   "metadata": {},
   "source": [
    "#### Example: Independent Samples T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7317b-ae85-4eca-a241-ab990be068c6",
   "metadata": {},
   "source": [
    "$H_0: \\bar{X} - \\bar{Y} = 0$\n",
    "\n",
    "$H_A: \\bar{X} - \\bar{Y} \\neq 0$\n",
    "\n",
    "Imagine that comprehension test scores range $0$ to $100$. $X_N$ and $X_Q$ indicate the comprehension scores in the noisy and quiet conditions.\n",
    "\n",
    "$X_N = [60, 52, 90, 20, 33, 95, 18, 47, 78, 65]$\n",
    "\n",
    "$X_Q = [65, 60, 84, 23, 37, 95, 17, 53, 88, 66]$\n",
    "\n",
    "$\\text{Diffs } (\\Delta) = [5, 8, -6, 3, 4, 0, -1, 6, 10, 11]$\n",
    "\n",
    "\n",
    "- $\\bar{x}_1 = 55.80$\n",
    "- $\\bar{x}_2 = 58.80$\n",
    "- $\\bar{d} = 3.00$\n",
    "\n",
    "$s = \\sqrt{ \\frac{\\sum_i (d_i - \\bar{d})^2}{n-1} } = 3.655$\n",
    "\n",
    "$t = \\frac{ \\bar{x}_1 - \\bar{x}_2 }{s / \\sqrt{n}} = \\frac{\\bar{d}-0}{s/\\sqrt{n}} = \\frac{3}{4.69 / \\sqrt{10}} = 2.023$\n",
    "\n",
    "The result is $t_9 = 2.023$, $p \\lt 0.074$. The test is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac409ce-0b99-4cf6-ac92-95edf3e3ef39",
   "metadata": {},
   "source": [
    "#### Example 2: Independent Samples T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ef077-ac05-4120-a52f-d359d0e71a10",
   "metadata": {},
   "source": [
    "Two financial organizations are about to merge, and are considering the level of service duplication. Two sales teams responsible for essentially identical products are compared by selecting samples from each and reviewing their respective profit contribution levels per employee over a period of two weeks. These are found to be $3,166$ and $2,240.4$, with estimated variance of $6,328.27$ and $221,661.3$ respectively. How do the two teams compare?\n",
    "\n",
    "- $n_1 = 4$\n",
    "- $n_2 = 9$\n",
    "- $\\bar{x}_1 = 3,166$\n",
    "- $\\bar{x}_2 = 2,240.4$\n",
    "- $s_1^2 = 6,328.67$\n",
    "- $s_2^2 = 221,663.3$\n",
    "\n",
    "$t = 5.72$ ($\\nu = 9$ when rounded)\n",
    "\n",
    "Critical value $t_{9;0.025} = 2.26$.\n",
    "\n",
    "We reject the $H_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6300e3d-8172-4612-b145-9e3414bab1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58dd5271-ae38-400a-a6d7-1fc352bc6108",
   "metadata": {},
   "source": [
    "### Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b78f7-ddd1-45dc-8b9d-660d844d3812",
   "metadata": {},
   "source": [
    "The chi-square test is a one-sided test used for categorical features. Assume that for some categorical characteristic, the number of individuals in each of k categories has been counted. We want to determine if the numbers in the categories are significantly different from hypothetical numbers. \n",
    "\n",
    "Unless otherwise specified, the null hypothesis assumes all proportions are equal, and the alternative hypothesis is that there is at least one difference. The chi-square test statistic compares the observed frequency distribution of each observation $O$ with the expected frequency distribution $E$, derived from the marginal probabilities on the assumption that characteristics are independent.\n",
    "\n",
    "<h4>$\\chi^2 = \\frac{\\sum_i [y_i - E(y_i)]^2}{E(y_i)}$</h4>\n",
    "\n",
    "- $\\sum_i$ is a summation over all observations $y_i$\n",
    "- $y_i$ is an individual observation\n",
    "- $E(y_i)$ is the expected value of observation $y_i$\n",
    "\n",
    "The chi-square statistic has $k-1$ degrees of freedom, where $k$ is the number of categories. The number of observations in each category should be at least $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd84996-3db5-4c27-bc37-cb37dc31d605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ad6bc7-56d8-4b96-a64a-9125a82f63d2",
   "metadata": {},
   "source": [
    "#### Example: Chi-Square Test for Goodness of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3261b26-6fab-4c9a-8f42-1b4da1bb9d29",
   "metadata": {},
   "source": [
    "The expected proportions of white, brown, and pied rabbits in a population are $0.36$, $0.48$, and $0.16$ respectively. In a sample of $400$, there were $140$ white, $240$ brown, and $20$ pied. Are the proportions in the sample different from expected?\n",
    "\n",
    "<h4>$\\chi^2 = \\frac{(140-144)^2}{144} + \\frac{(240-192)^2}{192} + \\frac{(20-64)^2}{64} = 42.361$</h4>\n",
    "\n",
    "The critical value $\\chi^2_{2; 0.05}$ is $5.991$. Since the calculated $\\chi^2$ is greater than the critical value, it can be concluded that the sample is different from the population with a $0.05$ level of significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf00f9-f426-4d65-9a09-5a34e1c9d70b",
   "metadata": {},
   "source": [
    "#### Example: Chi-Square Test for Goodness of Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a143439-3bb2-4c83-9c73-b1cacceb51a6",
   "metadata": {},
   "source": [
    "A die is thrown 120 times. The observed number of occurrences of $i$ is denoted $O_i, i=1, \\ldots, 6$. The data is as follows:\n",
    "    \n",
    "- $O_1=25, O_2=17, O_3=15, O_4=23, O_5=24, O_6=16$\n",
    "\n",
    "- $E_1=20, E_2=20, E_3=20, E_4=20, E_5=20, E_6=20$\n",
    "\n",
    "$\\chi^2 = \\frac{\\sum_i [y_i - E(y_i)]^2}{E(y_i)} = \\frac{25}{20} + \\frac{17}{20} + \\frac{15}{20} + \\frac{23}{20} + \\frac{24}{20} + \\frac{16}{20} = 5.0$\n",
    "\n",
    "The critical value is $\\chi^2_{5;0.05} = 11.1$\n",
    "\n",
    "The calculated value is less than the critical value, hence, there is no indication that the die is unfair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28f65e-c8d6-480f-861d-74ee4a9c2fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614626f1-1902-47c4-abee-6e8c8f3c2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66823612-a512-4675-aba8-ccb2744eabea",
   "metadata": {},
   "source": [
    "<h4>$\\chi^2 = \\frac{\\sum_i [y_i - E(y_i)]^2}{E(y_i)}$</h4>\n",
    "\n",
    "<h4>$\\chi^2 = \\frac{(25-20)^2 + (17-20)^2 + (15-20)^2 + (23-20)^2 + (24-20)^2 + (16-20)^2}{20}$</h4>\n",
    "\n",
    "<h4>$\\chi^2 = 5.00$</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afbf32-7f07-47c1-9c80-60207479025f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1952d0c3-180c-43d6-adea-df9af63b74e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf42b3-97ea-4736-9825-19fb490f2e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7f4fbb-d295-4306-a028-d25209756d6d",
   "metadata": {},
   "source": [
    "#### Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a75bd1-b70e-4aac-82f5-9dddf74b4c8c",
   "metadata": {},
   "source": [
    "A point estimate is the simplest approach to estimating a population parameter, and a confidence interval is a point estimate $\\pm$ the margin of error. The wider the interval, the greater the confidence level. To be $95\\%$ confident means that there is a $95\\%$ chance that the mean is contained in the confidence interval, and that the method used to construct the interval will provide intervals that contain the population mean $95\\%$ of the time.\n",
    "\n",
    "Involved in the calculation of a confidence interval is a critical statistic, such as a z or t-statistic, representative of the confidence level. A z-statistic is assumed to be drawn from a normal distribution, and a t-statistic is assumed to be drawn from a t-distribution.\n",
    "\n",
    "<i>Confidence Interval with z-Statistic:</i>\n",
    "\n",
    "<h4>$CI = \\bar{x} \\pm z_{\\alpha} \\frac{s}{\\sqrt{n}}$</h4>\n",
    "\n",
    "<i>Confidence Interval with t-Statistic:</i>\n",
    "\n",
    "<h4>$CI = \\bar{x} \\pm t_{n-1} \\frac{s}{\\sqrt{n}}$</h4>\n",
    "\n",
    "- $\\bar{x}$ is the sample average\n",
    "- $z_{\\alpha}$ is a critical z-statistic for the given $\\alpha$ (significance) level\n",
    "- $\\alpha$ is the significance level ($0.05$ or $0.01$ is common)\n",
    "- $s$ is the sample standard deviation\n",
    "- $t_{n-1}$ is a critical t-statistic with degrees of freedom equal to $n-1$\n",
    "- $n$ is the number of observations\n",
    "\n",
    "This is not the same as a p-value. The p-value can be small while the confidence interval is large, and vice versa. It is also not the same thing as standard deviation. Standard deviation is a measure of variability within one sample, while a confidence interval provides a range that we expect our population parameter to fall into in future samples, by providing an uncertainty estimate of the true population mean. Increasing the sample size does not necessarily decrease standard deviation, but it does narrow a confidence interval.\n",
    "\n",
    "Assumptions of confidence intervals include:\n",
    "- Independence: random sampling; one observation does not affect another\n",
    "- Normality: that the data are normally distributed in the population\n",
    "- Known Standard Deviation: the sample standard deviation is a good approximation of the population standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b76d3-c340-42a1-be17-e2017c866371",
   "metadata": {},
   "source": [
    "## ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7ac98-2067-40bb-946d-b4d29d905b03",
   "metadata": {},
   "source": [
    "ANOVA quantifies the total amount of variability in a dataset, and determines how much is attributable to the independent (X) variables and how much is due to noise or unmeasured factors, before computing the ratio of explained to unexplained variability. It is to determine the effects of categorical independent variables on a numerical dependent (y) variable. If the independent variable is naturally numerical rather than categorical, you can discretize it into a relatively small number of bins, or consider using regression instead.\n",
    "\n",
    "ANOVA creates a table of factors and levels. Factors are the independent variables in the study, and levels are the distinct categories or groups within each factor. Assumptions of ANOVA include:\n",
    "- Independence: observations are sampled randomly; one observation does not affect another\n",
    "- Normality: the population data is approximately normally distributed\n",
    "- Homogeneity of Variance: comparable levels of variance across all levels of the independent variables\n",
    "- Absence of Multicollinearity: absence of redundance (correlation) in the independent variables\n",
    "- No Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0316b3c-2f82-415c-b234-d4dfb3630992",
   "metadata": {},
   "source": [
    "#### One-Way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a29ac5-3ace-4719-a4d0-de818e8a1f5c",
   "metadata": {},
   "source": [
    "An ANOVA with one factor and two levels is essentially equivalent to a t-test. Both tests are used to compare means between groups. In a t-test, you directly compare the means of two groups, while in ANOVA, you are assessing whether there is a significant difference in means among groups.\n",
    "\n",
    "$H_0: \\mu_1 = \\mu_2 = \\ldots$\n",
    "\n",
    "$H_A: \\mu_i \\neq \\mu_j$\n",
    "\n",
    "ANOVA relies on the sum of squares, which is very similar to variance. The only difference is that variance divides by $n-1$ (equivalent to factoring by $\\frac{1}{n-1}$.\n",
    "\n",
    "$$SS = \\sum_{i=1}^n (x_i - \\bar{x})^2$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af9be4-7b0e-4cf8-9eeb-4c9442cccf89",
   "metadata": {},
   "source": [
    "#### Partitioning Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a46d00-fb1e-4e6b-9150-6db9658b1ad9",
   "metadata": {},
   "source": [
    "The sum of squares, i.e. variance, is partitioned in three different ways:\n",
    "- $SS_{Total}$ compares every individual within every group to the global mean of the data \\bar{x}, computing the total variance.\n",
    "- $SS_{Between}$ looks at the mean within each level minus the global mean.\n",
    "- $SS_{Within}$ is also called the sum of squared errors, comparing the mean of each individual to the mean within the specific group.\n",
    "    \n",
    "<img src=\"img/SS_3.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "-  $x_{ij}$ is the observation indexed by individual $i$ in level $j$\n",
    "- $\\bar{x}$ is the global mean of the variable $x$\n",
    "- $\\bar{x_j}$ is the mean for the level indexed by $j$\n",
    "- $n_j$ is the number of individuals within the level indexed by $j$\n",
    "\n",
    "We then compute the mean of squares between, and the mean of squares within:\n",
    "    \n",
    "<h4>$\\text{MS}_{Between} = \\frac{\\text{SS}_{Between}}{\\text{df}_{Between}}$</h4>\n",
    "\n",
    "<h4>$\\text{MS}_{Within} = \\frac{\\text{SS}_{Within}}{\\text{df}_{Within}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc357e1-1428-4972-aa7d-91c47a26e895",
   "metadata": {},
   "source": [
    "#### The F-Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c4e86-9016-444a-b1be-60075f44f972",
   "metadata": {},
   "source": [
    "The ratio of variabilities (normalized to control for sample size) is called an F-statistic, and large F-values provide evidence against the null hypothesis $H_0$. The critical F-value for significance depends on the degrees of freedom.\n",
    "\n",
    "<img src=\"img/anova_one_way.png\" style=\"height: 175px; width:auto;\">\n",
    "\n",
    "The F test statistic and F distribution are calculated as the ratio of two variances. It has two parameters, which are a degrees of freedom measure for the numerator and a degrees of freedom measure for the denominator. As with the normal distribution and t-distribution, critical values from the F-distribution are used to determine whether the observed F-statistic is significant.\n",
    "\n",
    "<h4>$F = \\frac{MS_{Between}}{MS_{Within}}$</h4>\n",
    "\n",
    "The p-value and F-ratio don't actually tell you which groups are different, they only tell you there is a difference somewhere. So it's necessary to do subsequent data visualization and t-tests to determine exactly which groups and levels. A p-value of less than the significance level indicates that at least one level is statistically significantly different from the mean of at least one other level.\n",
    "\n",
    "We can also evaluate the ANOVA by calculating an $R^2$ value, as:\n",
    "\n",
    "<h4>$R^2 = \\frac{SS_{Between}}{SS_{Total}}$</h4>\n",
    "\n",
    "And we can also calculate an adjusted R^2 (to control for number of parameters), as:\n",
    "\n",
    "<h4>$\\text{Adjusted } R^2 = 1 - \\frac{ (1-R^2)(n-1) }{ N - k - 1 }$</h4>\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $k$ is the number of parameters (independent variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595bb86c-2460-4a0b-8f8b-c23615c27d7d",
   "metadata": {},
   "source": [
    "#### Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75601281-007b-4341-ae89-f5ec1eec7a62",
   "metadata": {},
   "source": [
    "With two-way ANOVA, we are looking at the potential for interaction. If a medication has a different effect depending on whether you are young or old, that is an interaction.\n",
    "\n",
    "The total variation is expressed as the sum of variation across individuals within each group, plus the variation across different levels within each factor plus the variation of the interaction between the factors.\n",
    "\n",
    "<img src=\"img/SS_5.png\" style=\"height: 450px; width:auto;\">\n",
    "\n",
    "- $x_{ijk}$ is an observation of the $i^{th}$ observation of levels B, $j^{th}$ observation of level A, and $k^{th}$ observation of level B\n",
    "- $\\bar{x}_j$ is the average among the $j^{th}$ level of factor A\n",
    "- $\\bar{x}$ is the global average of variable $x$\n",
    "- $\\bar{x}_k$ is the average among the $k^{th}$ level of factor B\n",
    "\n",
    "To further describe the terms on the left:\n",
    "\n",
    "- $SS_{Total}$ is the same concept of the sum of squares, and is really just the total variance of the dataset. \n",
    "\n",
    "- $SS_{Between}$ has two factors now because we have two factors in our design. A two-way ANOVA with three factors would have three SS_{Between}terms, and so on. For $SS_{Between}$, we're ignoring one of the factors, and computing the marginal variance. The multiplication by $bn$ in order to compute $SS_{A \\times B}$ is not big $N$, the number of observations in the sample, but rather by $n$, the number of observations in each level of factor $B$, multiplied by $b$, the number of levels in factor $B$. The reverse is true for factor $A$. So $bn$ and $an$ represent the total number of observations for the other factor.\n",
    "\n",
    "- $SS_{A \\times B}$ is the interaction between A and B. We take the individual cell mean and subtract the marginal mean from factor B and the marginal mean from factor A within each level, and then add this to the total mean across the entire dataset.\n",
    "\n",
    "- $SS_{Within}$ is sometimes called the sum of squared errors. We're subtracting individuals from their mean, and then we compute the variance, and the key difference is that we have the mean of each cell instead of the entire dataset.\n",
    "\n",
    "The two-way ANOVA table is as follows:\n",
    "\n",
    "<img src=\"img/anova_two_way.png\" style=\"height: 250px; width:auto;\">\n",
    "\n",
    "If $p \\lt 0.05$, at least one level [for the group?] is significantly different from at least one other level. Determining which group requires data visualization and follow-up t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc950ab-1d52-40e1-933c-3ef89025fcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bace3-3e80-485d-8acd-b38f786ed013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bcebd-a6eb-4ab9-aa1c-31d4a60b55ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ec8df66-1c83-4f4b-898c-434ca56b8398",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232bbc1-1dd2-4c6c-be1c-e0e8ca0125a0",
   "metadata": {},
   "source": [
    "Linear regression is like correlation, but extendable to multiple independent variables, whereas correlation is only defined for two. Linear regression takes a set of inputs $X$ and a set of outputs $y$, and models a linear relationship by which the input maximally explains the output. For a single variable, the relationship is written as:\n",
    "\n",
    "<p>$y = \\beta_0 + \\beta_1 x + \\epsilon$</p>\n",
    "    <ul>\n",
    "        <li>$y$ is a vector of output from the regression function</li>\n",
    "        <li>$\\beta_0$ is an estimated intercept value</li>\n",
    "        <li>$\\beta_1$ is the estimated coefficient for the $x$ variable</li>\n",
    "        <li>$x$ is a vector of values</li>\n",
    "        <li>$\\epsilon$ is random error that cannot be explained by the model</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da391fe0-be91-49bb-8c69-3459b00e7cb7",
   "metadata": {},
   "source": [
    "#### Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a6139-9542-4b05-9eb6-6961ba905617",
   "metadata": {},
   "source": [
    "When multiple independent variables are being considered, we call it multiple regression. Each independent variable receives a coefficient (with a confidence interval) that describes the variable's contribution to the model, and predictions are created as a weighted sum of feature inputs.\n",
    "\n",
    "<p>$y = \\beta_0 + \\beta_1 x1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$</p>\n",
    "    <ul>\n",
    "        <li>$y$ is a vector of output from the regression function</li>\n",
    "        <li>$\\beta_0$ is an estimated intercept value</li>\n",
    "        <li>$\\beta_p$ is the estimated coefficient of the independent variable $x_p$</li>\n",
    "        <li>$x$ is a matrix of observations containing multiple features</li>\n",
    "        <li>$x_p$ is the independent variable indexed by $p$</li>\n",
    "        <li>$\\epsilon$ is random error that cannot be explained by the model</li>\n",
    "    </ul>\n",
    "\n",
    "One advantage of regression models is that they are highly interpretable. The features and weights can be interpreted as such:\n",
    "- Numerical Feature: increasing the numerical feature by one unit changes the estimated outcome by the value of its weight.\n",
    "- Binary Feature: changing the feature from the reference category to the other category changes the estimated outcome by the feature's weight.\n",
    "- Categorical Feature: the interpretation of each category is the same as the interpretation for binary features.\n",
    "- Intercept $\\beta_0$: the intercept is the predicted outcome of an instance where all features are at their mean value.\n",
    "\n",
    "Another advantage of regression is that it provides standard errors of the coefficients. Assumptions apply, such as a lack of correlated independent ($X$) features, however these assumptions are often violated with minimal consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655017ee-4ff0-40be-94f8-7f360eaf01f1",
   "metadata": {},
   "source": [
    "#### Evaluating Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a7c2f-2a80-4b6d-ab90-7fd8814ca214",
   "metadata": {},
   "source": [
    "A regression model can be evaluated as a single statistical object, such as by an F-test or Adjusted R^2. You can also evluate individual regressors (independent variables) using t-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b6abe-97fb-4292-912e-90d5d6a5bcc6",
   "metadata": {},
   "source": [
    "##### Sum of Squared Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f692c-cb44-4183-91e3-a2fb77662c82",
   "metadata": {},
   "source": [
    "<img src=\"img/SS_3_reg.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "- $SS_{Total}$ is the total variation in the dataset around its mean (when divided by d.f., it's equal to variance)\n",
    "- $SS_{Model}$ is the total variation of the predicted data around the mean\n",
    "- $SS_{\\epsilon}$ is the total variation of the predicted data relative to the observed data\n",
    "\n",
    "The three sum of squared terms are related to each other as in ANOVA, in that SS_{Total} is partitioned into the sum of two sources of variability, explained and unexplained.\n",
    "\n",
    "$SS_{Total} = SS_{Model} + SS_{\\epsilon}$\n",
    "\n",
    "Two methods for evaluating model fit, F-ratio (i.e., F-statistic) and $R^2$, are based on comparing these SS terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d735c-8866-4811-bb2d-a18177544542",
   "metadata": {},
   "source": [
    "#### F-Statistic for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13657d-9949-48da-9c5a-3dd92749ddb2",
   "metadata": {},
   "source": [
    "In the case of regression, the F-ratio is defined as:\n",
    "    \n",
    "<h4>$F_{(k-1,N-k)} = \\frac{ SS_{Model}~/~(k-1) }{ SS_{\\epsilon}~/~(N-k) }$</h4>\n",
    "\n",
    "- $k$ is the number of parameters in the regression model, including the intercept\n",
    "- $k-1$ is the degrees of freedom for the numerator of the F-statistic\n",
    "- $N-k$ is the degrees of freedom for the denominator of the F-statistic\n",
    "\n",
    "The better the model fits the data, the smaller the $SS_{\\epsilon}$ term, which increases the F-ratio. As $k$ gets larger, the numerator shrinks while the denominator increases, decreasing the F-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078ad60-5dcb-4222-9b2d-6c8254ec4ec2",
   "metadata": {},
   "source": [
    "#### Evaluating Individual Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e706ee-7ecd-4880-93d9-a227f90f09bf",
   "metadata": {},
   "source": [
    "If the model F-ratio is not statistically significant, it is inappropriate to evaluate individual regressors (you should not interpret a $p \\lt 0.05$ regressor in a model that is a non-significant fit to the data). Individual regressors are evaluated using a t-value, where the null hypothesis is that the coefficient is not different from 0. i.e., the null hypothesis is the $\\beta=0$. A t-statistic is a ratio of the mean effect to its standard error.\n",
    "\n",
    "<h3>$t_{N-k} = \\frac{\\beta_j}{SE(\\beta_j)} = \\frac{\\beta_j}{s_{\\beta_j} ~/~ \\sqrt{n}}$</h3>\n",
    "\n",
    "- $t_{N-k}$ is a t-statistic with $N-k$ degrees of freedom\n",
    "- $\\beta_j$ is the coefficient of the regressor indexed by $j$\n",
    "- $s_{\\beta_j}$ is the sample standard deviation for the coefficient of the regressor indexed by $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d19d8-50a9-497d-bb1f-4ae93ba29622",
   "metadata": {},
   "source": [
    "#### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ec975-ada3-4b29-bf8a-5bd1b2a944dc",
   "metadata": {},
   "source": [
    "Polynomial regression is used to model curves. In a polynomial regression, the columns in the design matrix are the x-axis value raised to increasing powers. i.e., the first column is $x^0$ (all ones), the second is $x^1$, the third is $x^2$, and so on. It is still considered a lienar regression model, because the model comprises scalar multiplication and addition and the $\\beta$ coefficients are estimated using linear methods.\n",
    "\n",
    "$y = \\beta_0 x^0 + \\beta_1 x^1 + \\ldots + B_k x^k$\n",
    "\n",
    "- $y$ is the dependent variable\n",
    "- $\\beta_k$ is the coefficient for the regressor indexed by $k$\n",
    "- $x$ is an independent variable raised to an increasing power for each term in the equation\n",
    "\n",
    "Models with higher orders will tend to fit the data better, but have a higher risk of overfitting. One solution is to compare them using the Bayes Information Criterion.\n",
    "\n",
    "$BIC_k = n ~ln (SS_{\\epsilon}) + k ~ln (n)$\n",
    "- $BIC_k$ is the Bayes Information Criterion for a model with $k$ regressors (independent variables)\n",
    "- $n$ is the number of observations\n",
    "- $ln$ is the natural log, meaning a logarithm of base $e$, where $e$ is Euler's constant\n",
    "- $SS_{\\epsilon}$ is the sum of squared errors\n",
    "\n",
    "The lower the BIC, the more theoretically optimal the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e24f39-51c8-454a-87d0-63e85729b899",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3f38b-532a-4abf-b561-c178b6779ee4",
   "metadata": {},
   "source": [
    "Logistic regression is an extension of linear regression toward classification, and models the probabilities for classification problems. The model inherently solves binary classification problems, however extensions to integrate multiple classes exist.\n",
    "\n",
    "Linear regression does not work for classification, because it does not output probabilities, but rather a line or hyperplane that minimizes the distance between itself and the points. A linear model extrapolates and gives values lower than 0 and greater than $1$, which can not be treated as probabilities. \n",
    "\n",
    "Logistic regression uses the logistic function to squeeze the output of a linear equation to between $0$ and $1$.\n",
    "\n",
    "<h5>$logistic(x) = \\frac{1}{1+e^{-x}}$</h5>\n",
    "\n",
    "- $logistic(x)$ is the input $x$ transformed into the output of the function\n",
    "- $x$ is an instance of input\n",
    "- $e$ is Euler's constant, an irrational number starting with $2.7818...$ \n",
    "\n",
    "(**link to appendix**)\n",
    "\n",
    "The $x$ in the logistic function is recognizable as the linear regression model:\n",
    "\n",
    "$P(y^{(i)} = 1) = \\frac{1}{ 1 + exp(-(\\beta_0 + \\beta_1 x_1^{(i)} + \\ldots + \\beta_p x_p^{(i)}) ) }$\n",
    "\n",
    "- $P(y^{(i)} = 1)$ is the probability that an instance of $y$ equals the target class\n",
    "- $exp()$ is equivalent to saying Euler's constant $e$ exponentiated to the expression in brackets\n",
    "\n",
    "(**link to appendix**)\n",
    "\n",
    "The interpretation of weights in logistic regression differs from the interpretability of weights in linear regression, because the weighted sum is transformed into a probability. We can reformulate the equation for the interpretation so that only the linear term is on the right side of the formula.\n",
    "\n",
    "$ln \\left( \\frac{P(y=1)}{P(y=0)} \\right) = \\beta_0 + \\beta_1 x_1^{(i)} + \\ldots + \\beta_p x_p^{(i)}$\n",
    "\n",
    "- $P(y^{(i)} = 1)$ is the probability that an instance of $y$ equals the target class\n",
    "- $P(y^{(i)} = 0)$ is the probability that an instance of $y$ equals the reference class\n",
    "- $ln$ is the base-e logarithm operation, where $e$ is Euler's constant\n",
    "\n",
    "We call the term in the brackets odds (the probability of one outcome divided by the probability of another), and wrapped in the logarithm, log-odds. A change in $x$ by one unit increases the log-odds ratio by the value of the corresponding weight, $\\beta_j$. Another, perhaps more intuitive way to interpret the weight $\\beta_j$ is that a change in $x$ by one unit increases the regular odds by $exp(\\beta_j)$. \n",
    "\n",
    "The components of the model can be interpreted as such:\n",
    "\n",
    "- Numerical Feature: if you increase the value of feature $x_j$ by one unit, the estimated odds change by a factor of $exp(\\beta_j)$.\n",
    "\n",
    "- Binary Categorical Feature: changing the feature $x_j$ from the reference category to the other category changes the estimated odds by a factor of $exp(\\beta_j)$.\n",
    "\n",
    "- Categorical Features: can be one-hot encoded (**link/appendix**) so that the binary categorical feature interpretation applies to each class.\n",
    "\n",
    "- Intercept $\\beta_0$: when all numerical features are zero and the categorical features are at the 'reference category' the estimated odds are $exp(\\beta_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab44381-c001-4e21-813c-e6389235c1fd",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5676f-17d6-46a3-ad3a-0ff3592f40af",
   "metadata": {},
   "source": [
    "#### Error Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00651ec1-e73e-44bc-a515-af45cdf04e2c",
   "metadata": {},
   "source": [
    "In a trial or experiment, an effect is either present or absent in each response, and there are four possible outcomes:\n",
    "\n",
    "1. A true positive a.k.a. a hit\n",
    "2. A false positive, a.k.a. a type I error or false alarm\n",
    "3. A true negative, a.k.a. a correct rejection\n",
    "4. A false negative, a.k.a. a type II error, or miss\n",
    "\n",
    "A confusion matrix represents these quantities visually.\n",
    "\n",
    "<img src=\"img/error_types.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=\"img/conf_matrix2.png\" style=\"height: 80px; width:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c877040-9713-4459-9271-c588ca2cc687",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0d9de-f43a-4994-9e52-1210d364048e",
   "metadata": {},
   "source": [
    "There are a number of metrics that can be calculated using the values from the confusion matrix, including accuracy:\n",
    "\n",
    "Accuracy is calculated as follows:\n",
    "\n",
    "$Accuracy = \\frac{ TP + TN }{ TP + FP + TN + FN }$\n",
    "\n",
    "The classification error rate is the inverse of classification accuracy:\n",
    "\n",
    "$Error Rate = \\frac{ FP + FN }{ TP + FP + TN + FN }$\n",
    "\n",
    "But used on a dataset with imbalanced classes, such as when 90% of observations belong to the same class, an unskilled model can provide 90% accuracy just by blindly picking the same class each time. To improve the accuracy further, you may need to use one of the other classification metrics, and in some scenarios, you will find one or more of the classification metrics to be more important than accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43067c-648f-4d0f-bd93-14cb9385e0e9",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b7662-3447-4547-9385-0c882eccda9b",
   "metadata": {},
   "source": [
    "Precision is the ratio of true positives to predicted positives. It is most used when there is a high cost for having false positives. Junk-mail classifiers should have a high degree of precision, so that they do not misclassify important emails as junk.\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169234f2-2168-4bbe-bd8f-9699fef1ba93",
   "metadata": {},
   "source": [
    "#### Sensitivity, a.k.a. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ceae94-2ea4-4814-a14a-e777c1a97266",
   "metadata": {},
   "source": [
    "Sensitivity is important when concerned with identifying positive outcomes and the cost of a false positive is low. If predicting whether a patient has cancer, it is important that sensitivity be high so that we can capture as many positive cases as possible.\n",
    "\n",
    "$Sensitivity = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64954287-0709-4fe0-a5dd-e5211f429e2f",
   "metadata": {},
   "source": [
    "#### Specificity (a.k.a. TNR, True Negative Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504a3e8-428d-4b0b-96f7-0580070c6004",
   "metadata": {},
   "source": [
    "Specificity is the ratio of true negatives to all negative outcomes. This is of interest if you are concerned about the accuracy of your negative rate and there is a high cost to a positive outcome. An example would be if you are an auditor looking over financial transactions and a positive outcome would mean a one-year investigation, but not finding one would cost very little.\n",
    "\n",
    "$Specificity = \\frac{TN}{TN+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d2c13-f63a-4ad8-bc81-df8736928b37",
   "metadata": {},
   "source": [
    "**link to appendix for more**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617491d1-156f-46f3-ad10-802eebc2baea",
   "metadata": {},
   "source": [
    "#### F1-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f733b9-f4f7-40c7-9f7f-dfa7ae98c7bb",
   "metadata": {},
   "source": [
    "There are also measures that blend multiple classification metrics into one, such as the F-Measure, a.k.a. the F-Score or F1-Score:\n",
    "\n",
    "$\\text{F-Measure} = \\frac{ 2 ~\\times ~Precision ~\\times ~Recall }{ Precision ~+ ~Recall }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bbfc2-7940-4761-878d-1f1daeb24985",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647e111-6b25-4c02-a2bb-e9944fa96767",
   "metadata": {},
   "source": [
    "Models are sensitive to the data they are trained on - if not, it can be considered to under-fit the data. If the model is too sensitive, or the data is misrepresentative of the population, then the model can over-fit, producing high training accuracy but fail to generalize well toward new data. If you are comparing several models, such as a series of regression models fit with different combinations or transformations of features, then it is helpful to employ a validation strategy which holds back some of the data for a post-training evaluation of the model on this unseen validation or test set.\n",
    "\n",
    "This is called cross-validation, and with a little automation, we could repeat the process several times, each time shuffling the data and choosing a new holdout-set at random. This is called k-fold cross-validation and provides an even better idea of how well a model will generalize on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b95432-6559-418d-989b-7fa3476e8083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fbdd6-253a-470d-9908-222c6d5357cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14688f-9891-4c44-8d08-9c41ffa87081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc212a-0897-414f-a069-28f14cebdd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18bd51ad-8509-43a5-946a-4eb3865fd5c1",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772e2e5-9d6d-4976-8289-df758e05255b",
   "metadata": {},
   "source": [
    "#### Logarithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dca23-51c0-4c0c-bdb1-3e8d3be90e81",
   "metadata": {},
   "source": [
    "$10^3 = 1000$, and we say that the base-10 logarithm (a.k.a. $log$) of 1000 is 3. It is the exponent to which the logarithm-base must be exponentiated in order to equal the other side of the equation. Different bases may be used, such as base-2, base-10, and base-e (Euler's constant), the irrational number starting with $2.7818$... A base-e logarithm, i.e., the natural logarithm, is often used in mathematics, and is written as $ln$.\n",
    "\n",
    "- $log_{10} ~1000 = 3$\n",
    "- $log_{2} ~8 = 3$\n",
    "- $log_e ~8 = log_{2.7818...} ~8 = ln ~8 = 2.07944$\n",
    "\n",
    "Logarithms increase and decrease monotonically, meaning that they preserve the order of the output among points. A logarithm upon a set of data in which each number is greater than the previous will result in another set of data in which each number is greater than the previous, just with a different shape and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d95d2-96e8-4add-acf7-3e3b55c49a53",
   "metadata": {},
   "source": [
    "#### Euler's Constant $e$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c423974-02b9-427d-bfca-92d00efa9d8a",
   "metadata": {},
   "source": [
    "$e$, like $\\pi$, is an irrational number, meaning the digits after the decimal neither end nor repeat. I.n Excel, it is rounded to 2.71828182845905.\n",
    "\n",
    "It is also similar to $\\pi$ in that it has profound importance in mathematics. Euler's formula, named after Leonhard Euler, establishes the relationship between trigonometric functions and complex numbers.\n",
    "\n",
    "$e^{ix} = cos ~x + i ~sin ~x$\n",
    "\n",
    "A special case of this formula known as Euler's identity links 5 fundamental mathematical concepts together (e,\\pi. the imaginary unit i, the number 1, and the number 0).\n",
    "\n",
    "$e^{i \\pi} + 1 = 0$\n",
    "\n",
    "In the realm of calculus, $e$ has an interesting property, which is that it is equal to its own <a href=\"https://en.wikipedia.org/wiki/Derivative\">derivative</a>.\n",
    "\n",
    "Some additional resources on the subject are as follows:\n",
    "\n",
    "- https://www.3blue1brown.com/lessons/eulers-number\n",
    "- https://www.3blue1brown.com/lessons/eulers-formula-dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff53f76-3192-4de7-9cfd-7c40ec765aae",
   "metadata": {},
   "source": [
    "#### The exp() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a423cf-6a45-4cac-912d-2edce4d5be06",
   "metadata": {},
   "source": [
    "The expression in the brackets of an $exp()$ function becomes an exponent to Euler's constant, $e$. For example, $e^{-3}$ is equal to $exp(-3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dbfd1b-357a-4d2c-a4d5-f9d3987c1390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16380a8d-6f8b-4239-88a2-8204c9a3e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PDF of the Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd043b-d9eb-4b6d-b712-49d8f717230b",
   "metadata": {},
   "source": [
    "<h2>$f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{ \\frac{1}{2} \\left( \\frac{x-\\mu}{\\sigma} \\right)^2 }$</h2>\n",
    "\n",
    "$x \\in (-\\infty, \\infty)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7d34f0-ec70-4b68-8af1-526270956ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PDF of the t-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf732dd-0514-4d03-b663-8b3b72cd0fba",
   "metadata": {},
   "source": [
    "<h2>$\\frac{ \\Gamma((n+1)/2) }{ \\sqrt{n \\pi} \\Gamma(n/2) } (1 + x^2/n)^{-(n+1)/2}$</h2>\n",
    "$x \\in (-\\infty, \\infty)$\n",
    "\n",
    "- where $\\Gamma$ is the <a href=\"https://en.wikipedia.org/wiki/Gamma_function\">Gamma function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04efaf6-c32e-4328-9ef1-a1f3ffcaefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PDF of the F-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b85c09-8632-4954-b82f-7716cc23456d",
   "metadata": {},
   "source": [
    "The F-distribution is defined as the ratio of two chi-squared variables divided by their degrees of freedom:\n",
    "    \n",
    "<h2>$F = \\frac{X / \\text{df}_X}{Y / \\text{df}_Y}$</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a3689-2d1b-4321-b5c2-7dd7f8b0b863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
