{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ed9f9-4ca2-4ed0-8d1f-aa6c85d36e0f",
   "metadata": {},
   "source": [
    "# Essential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01534dc5-2bf9-4bcb-b51b-29a8760f62bb",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54efccc-1cd0-49de-93fc-5eab254bf3fe",
   "metadata": {},
   "source": [
    "Summarizing data is important to understanding it at scale, and descriptive statistics help us to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13340b-d8b9-4a8d-89cd-3984786cfbab",
   "metadata": {},
   "source": [
    "----\n",
    "<i>\"Statistics is a broad set of algorithms for transforming numerical data into a small set of interpretable values that describe the world.\"</i>\n",
    "- Modern Statistics, Mike X. Cohen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b4d1e-be35-4602-a23f-0553a25fe603",
   "metadata": {},
   "source": [
    "#### Mean, Median, and Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2b0a9-32c7-4f48-bb05-b9a092f56b69",
   "metadata": {},
   "source": [
    "The mean of a series of numbers is the average, the sum divided by the count.\n",
    "\n",
    "<h4>$\\bar{x} ~~=~~ \\frac{ \\sum_{i=1}^n x_i }{n} ~~= ~~\\frac{1}{n} \\sum_{i=1}^n x_i ~~=~~ n^{-1} \\sum_{i=1}^n x_i$</h4>\n",
    "\n",
    "- $\\bar{x}$ is the mean of the series of numbers $x$\n",
    "- $\\sum_{i=1}^n$ indicates a summation of all $x_i$ in $x$\n",
    "- $n$ is the number of observations\n",
    "- $i$ is the index of a particular data point of $x$\n",
    "- $x_i$ is a data point of $x$ with index $i$\n",
    "- $1/n$ is a multiplication equivalent to dividing by $n$\n",
    "- $n^{-1}$ is equal to $1/n$\n",
    "\n",
    "The median is the data point in the middle of the list when you sort the series of numbers $x$ by value. It can be preferable to the mean as a measure of central tendency, as it is unaffected by outliers.\n",
    "\n",
    "The mode is the value observed most frequently in the series of numbers, and can be tied amongst two or more values.\n",
    "\n",
    "With an unskewed Gaussian/normal distribution, the median and mode happen to be equal to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a34ef3-92b8-4c54-aca4-4a1f706a6c1c",
   "metadata": {},
   "source": [
    "#### Percentile and Quartile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30a598-846e-455b-af27-9b332e2d6ade",
   "metadata": {},
   "source": [
    "You can think of the concept of percentiles as a generalization of the concept of the median, as the median represents the value at the $50^{th}$ percentile. Other special cases of the percentile are the values $25\\%$ and $75\\%$ of the way down the list of sorted values, and together with the median, these are called quartiles, because they divide the data into four sections with an equal number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417cf94-0f50-4da5-aa77-4afa6ceb146d",
   "metadata": {},
   "source": [
    "#### Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e9796-4103-4313-95fe-cedc872ffb44",
   "metadata": {},
   "source": [
    "Probability is the soul of statistics, which is all about quantifying uncertainty. It helps us to quantify randomness, and by calculating probabilities, we can determine whether observed phenomena are statistically significant or likely due to chance. Probabilities are non-negative numbers between $0$ and $1$, and part of a series of probabilities that together sum to $1$, called a probability distribution.\n",
    "\n",
    "There are many types of probability distributions, and the math behind them gets pretty hairy. We will not delve into the various ways that these distributions are calculated, but a couple of articles on the subject can be found here:\n",
    "\n",
    "- <a href=\"https://github.com/pw598/Articles/blob/main/Probability%20Distributions%20I%20-%20Discrete%20Distributions.ipynb\">Probability Distributions I - Discrete Distributions</a>\n",
    "- <a href=\"https://github.com/pw598/Articles/blob/main/Probability%20Distributions%20II%20-%20Continuous%20Distributions.ipynb\">Probability Distributions II - Continuous Distributions</a>\n",
    "\n",
    "What's important to know is that probability distributions, and the random variables which represent them, are mathematical functions. A function transforms a series of $x$ values input into some output, for example, $f(x)=x^2$. A probability distribution, known by its probability density function (PDF), is a parametric function where the outputs are probabilities. By parametric, we mean that the function is defined partly in terms of variable parameters - like with the Gaussian/normal distribution, mean and variance.\n",
    "\n",
    "Several important concepts are agnostic to the type of distribution being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea58bff-4c58-4dc8-bb1d-0c8c4fd6ec63",
   "metadata": {},
   "source": [
    "#### Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc1b02-cbef-4a96-b1d9-82e6e09cefcd",
   "metadata": {},
   "source": [
    "The expected value of a probability distribution is a measure of its center, and though equal to the mean in the case of symmetrical distributions, it can be thought of as a broader generalization. It is the average outcome you would expect when repeating an experiment many times, and is calculated by summing the products of each possible outcome and its probability.\n",
    "\n",
    "When $p(x_i) = \\frac{1}{n}$, the expected value and average are equal. The expected value and average will also be equal when $p(x_i) \\neq \\frac{1}{n}$ but the distribution is symmetric.\n",
    "\n",
    "<h4>$E(X) = \\sum_i x_i \\cdot P(X=x_i)$</h4>\n",
    "\n",
    "- $E(X)$ is the expected value of $X$\n",
    "- $\\sum_i$ represents a sum over all indices $i$ of $X$\n",
    "- $i$ is the index of the value of $X$ being considered\n",
    "- $x_i$ is the value of $X$ for a given index\n",
    "- $\\cdot$ is the multiplication operation\n",
    "- $P(X=x_i)$ is the probability that $X$ equals $x_i$\n",
    "\n",
    "For a continuous distribution, we can use the above as a discrete approximation, but otherwise need to involve the concept of integrals from calculus (which we'll avoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6c166-d2c0-41f4-94e4-83d1e7f66c1d",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a39e01-344b-46c4-875f-16d52e6ae9c0",
   "metadata": {},
   "source": [
    "Variance is a measure of how much spread there is around the expected value, and is calculated as the average of squared differences from the expected value. Squared because we don't want negative differences to offset the positive ones, but rather blend together to give us an idea of average distance regardless of direction. A nuance is that when dealing with a sample rather than a population, we divide by $n-1$ instead of $n$.\n",
    "\n",
    "<i>Population Variance:</i>\n",
    "\n",
    "<h4>$\\sigma^2 = \\frac{\\sum(x - \\mu)^2}{n}$</h4>\n",
    "\n",
    "<i>Sample Variance:</i>\n",
    "\n",
    "<h4>$s^2 = \\frac{\\sum(x - \\bar{x})^2}{n-1}$</h4>\n",
    "\n",
    "- $\\sigma^2$ is the population variance\n",
    "- $s^2$ is the sample variance\n",
    "- $x$ is the value of an instance in the dataset\n",
    "- $\\mu$ is the population average value\n",
    "- $\\bar{x}$ is the sample average value\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7008c39-727f-42cd-9690-fef1b3a4a0e8",
   "metadata": {},
   "source": [
    "#### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0812e-5c1e-48d5-97d7-f635fd0ec1b2",
   "metadata": {},
   "source": [
    "The squared values of variance make for an interpretability issue - the units of variance are not in the same unit of measurement of the data. This is why we commonly speak in terms of standard deviation - the square root of variance - which brings us back to the same units as the data.\n",
    "\n",
    "<i>Population Standard Deviation:</i>\n",
    "\n",
    "<h4>$\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{\\sum(x - \\mu)^2}{n}}$</h4>\n",
    "\n",
    "<i>Sample Standard Deviation:</i>\n",
    "\n",
    "<h4>$s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum(x - \\bar{x})^2}{n-1}}$</h4>\n",
    "\n",
    "- $\\sigma$ is the population standard deviation\n",
    "- $s$ is the sample standard deviation\n",
    "- $x$ is the value of an instance in the dataset\n",
    "- $\\mu$ is the population average value\n",
    "- $\\bar{x}$ is the sample average value\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c71eb-2d3d-4ce2-bcb3-b4c6dd705750",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da06429-7aa9-4516-8d58-0aa3cdc9c739",
   "metadata": {},
   "source": [
    "You are likely familiar with the concept of correlation - the degree to which the behavior of one variable explains another. To understand correlation, it helps to understand covariance, the unscaled version of correlation. While correlation ranges $-1$ to $+1$, covariance ranges $-X$ to $+X$, and correlation is just scaled covariance.\n",
    "\n",
    "Covariance is the sum of the product of differences from the mean for two variables, scaled by the number of observations $n$, or $n-1$ if sampling. Two coinciding positive differences make for a large contribution to covariance, as do two coinciding negative differences, through multiplication.\n",
    "\n",
    "<i>Population Covariance:</i>\n",
    "\n",
    "<h4>$cov(x,y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n}$</h4>\n",
    "\n",
    "<i>Sample Covariance:</i>\n",
    "\n",
    "<h4>$cov(x,y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n-1}$</h4>\n",
    "\n",
    "- $x$ is a vector (series) of values\n",
    "- $y$ is a vector of values\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\bar{x}$ is the average value of $x$\n",
    "- $y_i$ is an individual data point of $y$\n",
    "- $\\bar{y}$ is the average value of $y$\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a34ebc-a0b9-4f50-b55f-3d9ee2ef07a4",
   "metadata": {},
   "source": [
    "Back to correlation - correlation is covariance with the numerator scaled by the product of standard deviations for the two variables, rather than $n$ or $n-1$.\n",
    "\n",
    "<h4>$r = \\frac{ \\sum (x_i - \\bar{x}) (y_i - \\bar{y}) }{ \\sqrt{\\sum(x_i - \\bar{x})^2} \\sqrt{\\sum(y_i - \\bar{y})^2} }$</h4>\n",
    "\n",
    "- $x$ is a vector of values\n",
    "- $y$ is a vector of values\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\bar{x}$ is the average value of $x$\n",
    "- $y_i$ is an individual data point of $y$\n",
    "- $\\bar{y}$ is the average value of $y$\n",
    "- $n$ is the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5cca1-0bc6-472f-b14b-e05beb0d1a68",
   "metadata": {},
   "source": [
    "#### Z-Scores and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9d11c-9333-4af3-9eb5-fa23a93a0837",
   "metadata": {},
   "source": [
    "Z-Scores indicate how many standard deviations away a data point is from the mean of the dataset.\n",
    "\n",
    "<h4>$z = \\frac{x_i - \\mu}{\\sigma}$</h4>\n",
    "\n",
    "- $z$ is the distance in standard deviations of $x_i$ from the mean of $x$, $\\mu$\n",
    "- $x_i$ is an individual data point of $x$\n",
    "- $\\mu$ is the average value of $x$\n",
    "- $\\sigma$ is the standard deviation of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fa8a3-d5bc-4443-a941-4dcf87232e30",
   "metadata": {},
   "source": [
    "Z-scores can be helpful for standardization, a form of scaling in which data points are expressed in terms of the number of standard deviations away from the mean.\n",
    "\n",
    "Another way to scale data is with min/max scaling, which follows the following formula:\n",
    "\n",
    "<h4>$scaled ~x_i = \\frac{x_i - min(x)}{max(x) - min(x)}$</h4>\n",
    "\n",
    "- $scaled ~x_i$ is the proportion of range from the minimum to maximum of the vector $x$\n",
    "- $x_i$ is an individual instance of $x$\n",
    "- $min(x)$ is the minimum value of $x$\n",
    "- $max(x)$ is the maximum value of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c8ec0-bd60-486e-a7eb-1bc89bc13477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaab1d-a03e-4ec8-ac95-f8575d967bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a7372-aa45-4cc1-beee-ad087bb8a68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3e86e-5277-44b1-86ec-752c4de92f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b966a5-5400-4cf4-bee5-7d4cd681d494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c235f258-470d-4db1-a4a9-72e23966b9c7",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509668d-a5ff-4154-a68b-e8155b690e6a",
   "metadata": {},
   "source": [
    "#### Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520a3a-d1b9-4225-bf42-8e73a01bd220",
   "metadata": {},
   "source": [
    "Inferential statistics refers to algorithms that are applied to one or more sample datasets in order to test whether the descriptive statistics are likely to generalize to another dataset.\n",
    "\n",
    "Probability distributions provide the mathematical framework for describing the likelihood of events in a random process or experiment. Methods like maximum likelihood estimation use analytical functions to find parameters that best fit observed data to a theoretical distribution.\n",
    "\n",
    "----\n",
    "<i>It would not be controversial to claim that inferential statistics is basically just applied probability.</i>\n",
    "- Modern Statistics, Mike X. Cohen\n",
    "----\n",
    "\n",
    "Going forward, we will assume the Gaussian/normal distribution, or at times, the t-distribution. The parameters of the Gaussian distribution are mean and variance, and the parameter of the t-distribution is degrees of freedom, set to $n-1$ (where $n$ is the number of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cfce9-7cda-4638-baa6-53d64dd1541e",
   "metadata": {},
   "source": [
    "#### Sampling and Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025790d-3a05-4483-9e2c-e5983fda69a8",
   "metadata": {},
   "source": [
    "Sample size is the number of observations in a dataset. It is often denoted by N, but also by n. We can generalize about the population from a sample only if the sample is random, representative, and sufficiently large. An appropriate sample size depends on a number of factors, including effect size, variability in the sample and population, how closely the sample characteristics match the population, and how the samples were collected.\n",
    "\n",
    "If testing the effect of an experiment, we expect to see that a sample mean from the experiment is unlikely to come from the assumed population probability distribution, meaning that the difference in stimulus/environment/etc. has had a <i>significant</i> effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8eacd-660c-46dc-bebd-1a363e928bfd",
   "metadata": {},
   "source": [
    "#### p-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9529b60-4334-4efa-9bfb-46f1ab6fcd9d",
   "metadata": {},
   "source": [
    "A p-value (probability value) is the probability that an effect you observe in data is actually due to chance and not a true effect. It is the probability of obtaining an observed test statistic if the null hypothesis (discussed next) is true. A significance level $\\alpha$ reflecting the maximum threshold for the p-value is commonly selected to be $0.05$ or $0.01$. It is when the p-value is lower than the significance level $\\alpha$ that we consider a result not likely to be due to random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b1aee-14d3-4b65-85d4-2fd77c1882e5",
   "metadata": {},
   "source": [
    "#### Null and Alternative Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d3eb7-7aec-49f7-87cb-268baed6c1b5",
   "metadata": {},
   "source": [
    "A hypothesis is a falsifiable claim that requires verification, typically from experimental or observational data, and allows for predictions about future observations.\n",
    "\n",
    "The null hypothesis $H_0$ is that there is no difference between the sampled data and the population. The drug had no effect, the sale provided no uplift, etc. When the p-value is less than the significance level $\\alpha$, the null hypothesis $H_0$ is rejected in favor of the alternative hypothesis $H_A$ (or $H_1$). Rejecting the null hypothesis does not prove that the alternative hypothesis is true, but suggests there is sufficient enough evidence that the null hypothesis is unlikely to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e1ec1-8b64-4bf0-829e-091317813ffd",
   "metadata": {},
   "source": [
    "#### Critical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74cbe5-41ae-403e-8a76-24ffb7c97506",
   "metadata": {},
   "source": [
    "A critical value is defined in the context of the population distribution and a probability, and is used as a threshold for interpreting the result of a statistical test. The values in the population beyond the critical value are called the critical region or region of rejection.\n",
    "\n",
    "A one-tailed test has a single critical value, on the left or the right of the distribution, and if the calculated statistic is less or equally extreme than the critical value, the null hypothesis of the test fails to be rejected. A two-tailed test has two critical values, one on each side of the distribution, which is often assumed to be symmetrical. When using a two-tailed test, the significance level $\\alpha$ used in the calculation of critical values must be divided by two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be66f05-2e09-4da9-a4b9-a9147821180e",
   "metadata": {},
   "source": [
    "<img src=\"img/critical_region.png\" style=\"height: 400px; width:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917a9fc-bf26-4f92-9faf-067bc2d7b0f5",
   "metadata": {},
   "source": [
    "### The Z-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8189b91-cb6d-4373-8afa-28ebdf5f5dce",
   "metadata": {},
   "source": [
    "A z-test quantifies the probability of a number as or more extreme than another given number, when drawn from a normal distribution, given the data observed. Notable p-z combinations include:\n",
    "- 68.3% of data is between -1 and +1 standard deviations from the mean\n",
    "- 95.5% of data is between -2 and +2 standard deviations from the mean\n",
    "- 99.7% of data is between -3 and +3 standard deviations from the mean\n",
    "\n",
    "<img src=\"img/norm_dist.png\" style=\"height: 350px; width:auto;\">\n",
    "\n",
    "A z-test for a population mean investigates the significance of the difference between an assumed population mean $\\mu_0$ and a sample mean $\\bar{x}$. For  test, we assume to known the population variance $\\sigma^2$. If the variance is unknown (i.e., we don't assume it), then we use the t-test for a population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388f0dd-4905-463d-b3ca-edc71ea3de13",
   "metadata": {},
   "source": [
    "#### Z-Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3846002-5f13-4e1e-a54f-f939a0b7e4f6",
   "metadata": {},
   "source": [
    "From a population with assumed mean $\\mu_0$ and known variance $\\sigma^2$, a random sample of size $n$ is taken and the sample mean $\\bar{x}$ calculated. The test statistic, \n",
    "\n",
    "<h4>$z = \\frac{\\bar{x} - \\mu_o}{\\sigma / \\sqrt{n}}$</h4>\n",
    "\n",
    "- $z$ is the calculated test statistic\n",
    "- $\\bar{x}$ is the average value of variable $x$\n",
    "- $\\mu_0$ is the null hypothesis mean\n",
    "- $\\sigma$ is the standard deviation\n",
    "- $n$ is the number of observations\n",
    "\n",
    "may be compared with the standard normal distribution using either a one-tailed or two-tailed test (with critical region of size $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6152acc-acf1-4259-98b3-e26496a5979b",
   "metadata": {},
   "source": [
    "#### Z-Test for Population Mean Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dd9a8-ed90-41df-a615-3946ac4a3588",
   "metadata": {
    "tags": []
   },
   "source": [
    "A cosmetics filling process fills tubs of powder with $4$ grams on average with a standard deviation of $1$ gram. A sample of $9$ tubs are weighed, and the average weight is $4.6$ grams. What can be said about the filling process, at a significance level of $0.05$?\n",
    "\n",
    "- $\\bar{x} = 4.6$\n",
    "- $\\mu_0 = 4.0$\n",
    "- $\\sigma = 1.0$\n",
    "- $n = 9$\n",
    "\n",
    "<h4>$z = \\frac{4.6-4.0}{1 / \\sqrt{9}} = 1.8$</h4>\n",
    "\n",
    "The critical value $z_{0.05} = 1.96$. The z-stat is not as extreme as the critical z; our range of acceptance of the null hypothesis is $-1.96$ to $1.96$, and so we fail to reject the null hypothesis.\n",
    "\n",
    "However, if we are only concerned about over-filling and not under-filling, it becomes a one-tailed test instead of a two-tailed test, in which the acceptance region is now $z \\lt 1.645$. So now we reject the null hypothesis and reasonaly suspect that the tubs are being over-filled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca3c54-7045-48bc-a640-752cd79ac2af",
   "metadata": {},
   "source": [
    "### The T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279966c5-d2d7-4639-a121-49d8b5a39018",
   "metadata": {
    "tags": []
   },
   "source": [
    "The purpose of a t-test is to determine whether the mean of a sample is different from a specified null hypothesis $H_0$ value. There are three scenarios in which you would use a t-test:\n",
    "\n",
    "1. One-Sample T-Test: you have one data sample, and the objective is to determine if the sample mean significantly deviates from a predetermined $H_0$ value.\n",
    "\n",
    "2. Paired Samples T-Test: you have one group of individuals that were measured twice; for example, before and after a treatment.\n",
    "\n",
    "3. Independent Samples T-Test: you have two separate groups of individuals and want to determine whether the means of the two groups differ.\n",
    "\n",
    "The test statistic, \n",
    "\n",
    "<h4>$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$</h4>\n",
    "\n",
    "- $t$ is the calculated test statistic\n",
    "- $\\bar{x}$ is the average value of variable $x$\n",
    "- $\\mu_0$ is the null hypothesis mean\n",
    "- $s$ is the standard deviation\n",
    "- $n$ is the number of observations\n",
    "\n",
    "may be compared with the Student's t-distribution (Student was the pseudoname of the person who proposed it) with $n-1$ degrees of freedom, and may be one-tailed or two-tailed. The t-distribution approximates a normal distribution as the number of samples increases, but for lower numbers of samples, it is shorter and more spread out.\n",
    "\n",
    "<img src=\"img/t_dist.png\" style=\"height: 400px; width:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25323994-6a5d-4d3a-98f7-fd54b125396d",
   "metadata": {},
   "source": [
    "#### Degrees of Freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eeb9ee-b65c-44da-b0f1-0e2b016d3786",
   "metadata": {},
   "source": [
    "If I tell you that the average of three numbers is $4$, and that two of those numbers are $2$ and $3$, then you know that the third number is $7$. In other words, the statistic has $n-1=2$ degrees of freedom (d.f.).\n",
    "\n",
    "Using $n-1$ as the degree of freedom for one parameter (variable) generalizes to using $n-k$ in the multivariate caes, where $n$ is the number of observations and $k$ is the number of parameters.\n",
    "\n",
    "The degrees of freedom associated with a t-test is n-1 or n-2, depending on whether there is one group or two. The degrees of freedom associated with a correlation analysis is 2, and the degrees of freedom associated with a regression is n-k, where k is the number of variables. Degrees of freedom also play a role in ANOVA, which we will cover in depth later on.\n",
    "\n",
    "If we have a t-value of 1.6 with d.f. = 20, this is not statistically significant at \\alpha=0.05 because the area of the null hypothesis $H_0$ for t \\gt 1.6 is 0.0626, or 6.26%. With a two-tailed test, the p-value associated with t=1.6 is multiplied by 2, because each tail contains 6.26% of the total area, and the p-value doubles to 0.1252."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd80bb-9870-49b8-b404-b40e6bb5c9ac",
   "metadata": {},
   "source": [
    "#### T-Test for a Population Mean Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35409129-2e7b-4e22-b837-9534b462b353",
   "metadata": {},
   "source": [
    "A sample of $9$ plastic nuts yielded an average diameter of $3.1cm$ and estimated standard deviation of $1.0cm$. The population mean is assumed to be $4.0cm$. What does this say about the mean diameter of plastic nuts being produced?\n",
    "\n",
    "- $\\bar{x} = 3.1$\n",
    "- $\\mu_0 = 4.0$\n",
    "- $s = 1.0$\n",
    "- $n = 9$\n",
    "\n",
    "<h4>$t = \\frac{3.1 - 4.0}{1.0 / \\sqrt{9}} = 2.7$</h4>\n",
    "\n",
    "Our computed t value is $-2.7$ and the critical value is $t_{8; 0.025} = \\pm 2.3$. The acceptance region is $-2.3$ to $2.3$ (**reference table**), so we reject the null hypothesis and accept the alternative hypothesis that there is a difference between the sample and population means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d76f49-9774-4b40-ab44-85183ac89997",
   "metadata": {},
   "source": [
    "#### T-Test for Two Population Means - Method of Paired Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dac15-767b-4cd6-8555-dd73a6757941",
   "metadata": {},
   "source": [
    "This test is to investigate the significance of the difference between two population means, $\\mu_1$ and $\\mu_2$, making no assumption about the population variances. The observations for the two samples must be obtained in pairs. Apart from population differences, the observations in each pair should be carried out under identical or near-identical conditions.\n",
    "\n",
    "The differences $d_i$ are formed for each pair of observations. If there are $n$ such pairs of observations, we can calculate the variance of the differences by:\n",
    "\n",
    "<h4>$s^2 = \\sum_{i=1}^n \\frac{ (d_i - \\bar{d})^2 }{n-1}$</h4>\n",
    "\n",
    "- $s^2$ is the variance of the differences\n",
    "- $\\sum_{i=1}^n$ is a summation over the differences for each individual\n",
    "- $d_i$ is the difference (such as before vs. after) for the individual indexed by $i$\n",
    "- $\\bar{d}$ is the average differencel for all individuals\n",
    "\n",
    "The test statistic becomes:\n",
    "\n",
    "<h4>$t = \\frac{ \\bar{x}_1 - \\bar{x}_2 }{s / \\sqrt{n}}$</h4> \n",
    "\n",
    "- $t$ is the calculated test statistic\n",
    "- $\\bar{x}_1$ is the mean of the first group of observations\n",
    "- $\\bar{x}_2$ is the mean of the second group of observations\n",
    "- $s$ is the standard deviation, the square root of the variance calculation above\n",
    "- $n$ is the number of observations\n",
    "\n",
    "which follows the Student's t-distribution with $n-1$ degrees of freedom. The test may be one-tailed or two-tailed.\n",
    "\n",
    "To compare the efficacy of two treatments for a respiratory condition, $10$ patients are randomly selected and the treatments are administered. The patient then exercises until a maximum exercise rate is reached. The times for these are compared...\n",
    "\n",
    "- $d = \\bar{x}_1 - \\bar{x}_2 = -0.1$\n",
    "- $s = 2.9$\n",
    "- $n = 10$\n",
    "\n",
    "<h4>$t = \\frac{-0.1}{2.9 / \\sqrt{10}} = -0.11$</h4>\n",
    "\n",
    "The critical t-statistic is $t_{9; 0.025} = 2.26$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251a2d2-21f8-4f04-a84d-1c988bcc9724",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060982e7-470b-45bc-a767-a86c31cad1f3",
   "metadata": {},
   "source": [
    "#### Independent Samples T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05c740-2d00-432c-8e76-a33b151fa941",
   "metadata": {},
   "source": [
    "The independent samples t-test evaluates whether the means of two groups significantly differ. The sample sizes may differ between the two groups. The formula that separates the sample sizes and variances is called Welch's t-test.\n",
    "\n",
    "<h3>$t = \\frac{ \\bar{x} - \\bar{y} }{ \\sqrt{ \\frac{s_x^2}{n_x} + \\frac{s_y^2}{n_y} } }$</h3>\n",
    "\n",
    "- $t$ is the calculated t-statistic\n",
    "- $\\bar{x}$ is the average of variable $x$\n",
    "- $\\bar{y}$ is the average of variable $y$\n",
    "- $s_x$ is the sample standard deviation for $x$\n",
    "- $s_y$ is the sample standard deviation for $y$\n",
    "- $n_x$ is the number of observations in variable $x$\n",
    "- $n_y$ is the number of observations in variable $y$\n",
    "\n",
    "If the variances are roughly equal, the degrees of freedom are n_x + n_y - 2. If unequal, the formula is a little complicated:\n",
    "\n",
    "(**move this to appendix**)\n",
    "\n",
    "<h3>$d.f. = \\frac{ (s_1^2~/~n_1 +s_2^2~/~n_2)^2 }{ \\frac{s_1^2}{n_1^2(n-1)} + \\frac{s^2}{n_1^2(n_1-1)} }$</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d21c8-dfa7-4e07-9dda-96921ade2996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1892c33-08ab-4700-8171-cd27899f0302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d285b2f-8c12-4a84-8be2-e0b1a03f8c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab1190f7-0cad-49ec-b9b6-108dd85272ce",
   "metadata": {},
   "source": [
    "#### Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210b1ff-eb79-4426-8c04-c6d59bd20b66",
   "metadata": {},
   "source": [
    "A confidence interval provides a range of values within which a population parameter may occur, given a sample.\n",
    "\n",
    "----\n",
    "\"A 95% confidence interval estimates a range for a parameter, such that if we were to take repeated samples, we would expect the true population mean to be contained within this interval in 95% of those samples.\n",
    "\n",
    "Involved in the calculation of a confidence interval is a critical statistic, such as a z-score or t-statistic, representative of the confidence level.\n",
    "\n",
    "<i>Confidence Interval with z-Statistic:</i>\n",
    "\n",
    "<h4>$CI = \\bar{x} \\pm z_{\\alpha} \\frac{s}{\\sqrt{n}}$</h4>\n",
    "\n",
    "<i>Confidence Interval with t-Statistic:</i>\n",
    "\n",
    "<h4>$CI = \\bar{x} \\pm t_{n-1} \\frac{s}{\\sqrt{n}}$</h4>\n",
    "\n",
    "- $\\bar{x}$ is the sample average\n",
    "- $z_{\\alpha}$ is a critical z-statistic for the given $\\alpha$ (significance) level\n",
    "- $\\alpha$ is the significance level ($0.05$ or $0.01$ is common)\n",
    "- $s$ is the sample standard deviation\n",
    "- $t_{n-1}$ is a critical t-statistic with degrees of freedom equal to $n-1$\n",
    "- $n$ is the number of observations\n",
    "\n",
    "This is not the same as a p-value. The p-value can be small while the confidence interval is large, and vice versa. It is also not the same thing as standard deviation. Standard deviation is a measure of variability within one sample, while a confidence interval provides a range that we expect our population parameter to fall into in future samples, by providing an uncertainty estimate of the true population mean. Increasing the sample size does not necessarily decrease standard deviation, but it does narrow a confidence interval.\n",
    "\n",
    "Assumptions of confidence intervals include:\n",
    "- Independence: random sampling; one observation does not affect another\n",
    "- Normality: that the data are normally distributed in the population\n",
    "- Known Standard Deviation: the sample standard deviation is a good approximation of the population standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba073a-f70e-4e79-8d42-493f379a7047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc3575-8712-49d2-a8c5-aed44896fde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8993f-5e2a-481b-ba57-4caf37457b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d619e-83c3-42b5-a549-ce21ff7fe5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9908d6-bd07-4c77-9f68-1845b2154a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1ea71-a783-43c1-88bf-eda8bac23858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5bf44-8354-46d9-8e6b-50a29f22d2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b001b8-1851-406d-801c-28c06bb2332a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b0f12-2f1a-4f36-9cd2-e1ed029e704b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e599d015-8aec-45c3-aa38-043474277752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514efb0-9185-42d0-8b54-539735b3404b",
   "metadata": {},
   "source": [
    "Linear regression is like correlation, but extendible to multiple independent variables, whereas correlation is only defined for two. Linear regression takes a set of inputs $X$ and a set of outputs $y$, and models a linear relationship by which the input maximally explains the output. For a single variable, the relationship is written as:\n",
    "\n",
    "<p>$y = \\beta_0 + \\beta_1 x + \\epsilon$</p>\n",
    "    <ul>\n",
    "        <li>$y$ is the numeric response for the instance in question</li>\n",
    "        <li>$\\beta_0$ is an estimated intercept value</li>\n",
    "        <li>$\\beta_1$ is the estimated coefficient for the $x$ variable</li>\n",
    "        <li>$x$ is a vector of values</li>\n",
    "        <li>$\\epsilon$ is random error that cannot be explained by the model</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfa725-7107-46b2-8e45-86d007b53a8d",
   "metadata": {},
   "source": [
    "#### Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641f211-3752-4475-bf5d-28f3641f4d7d",
   "metadata": {},
   "source": [
    "When multiple independent variables are being considered, we call it multiple regression. Each independent variable receives a coefficient (with a confidence interval) that describes the variable's contribution to the model, and predictions are created as a weighted sum of feature inputs.\n",
    "\n",
    "<p>$y = \\beta_0 + \\beta_1 x1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$</p>\n",
    "    <ul>\n",
    "        <li>$y$ is the numeric response for the instance in question</li>\n",
    "        <li>$\\beta_0$ is an estimated intercept value</li>\n",
    "        <li>$\\beta_p$ is the estimated coefficient of the independent variable $x_p$</li>\n",
    "        <li>$x$ is a matrix of observations containing multiple features</li>\n",
    "        <li>$x_p$ is the independent variable indexed by $p$</li>\n",
    "        <li>$\\epsilon$ is random error that cannot be explained by the model</li>\n",
    "    </ul>\n",
    "\n",
    "One advantage of regression models is that they are highly interpretable. The features and weights can be interpreted as such:\n",
    "- Numerical Feature: increasing the numerical feature by one unit changes the estimated outcome by the value of its weight.\n",
    "- Binary Feature: changing the feature from the reference category to the other category changes the estimated outcome by the feature's weight.\n",
    "- Categorical Feature: the interpretation of each category is the same as the interpretation for binary features.\n",
    "- Intercept $\\beta_0$: the intercept is the predicted outcome of an instance where all features are at their mean value.\n",
    "\n",
    "Another advantage of regression is that it provides standard errors of the coefficients. Assumptions apply, such as a lack of correlated independent ($X$) features, however these assumptions are often violated with minimal consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c2400-bf12-49ce-9c92-7764bc1c1ece",
   "metadata": {},
   "source": [
    "### Evaluating Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b02dc7-689d-4815-8e3f-e57db8adff98",
   "metadata": {},
   "source": [
    "A regression model can be evaluated as a single statistical object, such as by an F-test or Adjusted R^2. You can also evluate individual regressors (independent variables) using t-values.\n",
    "\n",
    "<img src=\"img/SS_3_reg.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "- $SS_{Total}$ is the total variation in the dataset around its mean (when divided by d.f., it's equal to variance)\n",
    "- $SS_{Model}$ is the total variation of the predicted data around the mean\n",
    "- $SS_{\\epsilon}$ is the total variation of the predicted data relative to the observed data\n",
    "\n",
    "The three sum of squared terms are related to each other as in ANOVA, in that SS_{Total} is partitioned into the sum of two sources of variability, explained and unexplained.\n",
    "\n",
    "$SS_{Total} = SS_{Model} + SS_{\\epsilon}$\n",
    "\n",
    "Two methods for evaluating model fit, $F-ratio$ and $R^2$, are based on comparing these SS terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c22e71-3896-4adf-8b12-79d08d430983",
   "metadata": {},
   "source": [
    "#### F-Statistic for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dce580-d408-43a3-9abe-c36761432450",
   "metadata": {},
   "source": [
    "In the case of regression, the F-ratio is defined as:\n",
    "    \n",
    "<h4>$F_{(k-1,N-k)} = \\frac{ SS_{Model}~/~(k-1) }{ SS_{\\epsilon}~/~(N-k) }$</h4>\n",
    "\n",
    "- $k$ is the number of parameters in the regression model, including the intercept\n",
    "- $k-1$ is the degrees of freedom for the numerator of the F-statistic\n",
    "- $N-k$ is the degrees of freedom for the denominator of the F-statistic\n",
    "\n",
    "The better the model fits the data, the smaller the $SS_{\\epsilon}$ term, which increases the F-ratio. As $k$ gets larger, the numerator shrinks while the denominator increases, decreasing the F-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db339625-5cb5-42f7-b432-ef353268bcc2",
   "metadata": {},
   "source": [
    "#### Evaluating Individual Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359dd6b-5745-4d4b-b6f3-f7825daacd23",
   "metadata": {},
   "source": [
    "If the model F-ratio is not statistically significant, it is inappropriate to evaluate individual regressors (you should not interpret a $p \\lt 0.05$ regressor in a model that is a non-significant fit to the data). Individual regressors are evaluated using a t-value, where the null hypothesis is that the coefficient is not different from 0. i.e., the null hypothesis is the $\\beta=0$. A t-statistic is a ratio of the mean effect to its standard error.\n",
    "\n",
    "<h3>$t_{N-k} = \\frac{\\beta_j}{SE(\\beta_j)} = \\frac{\\beta_j}{s_{\\beta_j} ~/~ \\sqrt{n}}$</h3>\n",
    "\n",
    "- $t_{N-k}$ is a t-statistic with $N-k$ degrees of freedom\n",
    "- $\\beta_j$ is the coefficient of the regressor indexed by $j$\n",
    "- $s_{\\beta_j}$ is the sample standard deviation for the coefficient of the regressor indexed by $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5997a1-ee53-45b2-83cc-3388f4372a0b",
   "metadata": {},
   "source": [
    "#### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9edcb-91f2-4c27-9529-ce220bd4a40e",
   "metadata": {},
   "source": [
    "Polynomial regression is used to model curves. In a polynomial regression, the columns in the design matrix are the x-axis value raised to increasing powers. i.e., the first column is $x^0$ (all ones), the second is $x^1$, the third is $x^2$, and so on. It is still considered a lienar regression model, because the model comprises scalar multiplication and addition and the $\\beta$ coefficients are estimated using linear methods.\n",
    "\n",
    "$y = \\beta_0 x^0 + \\beta_1 x^1 + \\ldots + B_k x^k$\n",
    "\n",
    "- $y$ is the dependent variable\n",
    "- $\\beta_k$ is the coefficient for the regressor indexed by $k$\n",
    "- $x$ is an independent variable engineered to increase in the order of exponent for each instance\n",
    "\n",
    "Models with higher orders will tend to fit the data better, but have a higher risk of overfitting. One solution is to compare them using the Bayes Information Criterion.\n",
    "\n",
    "$BIC_k = n ~ln (SS_{\\epsilon}) + k ~ln (n)$\n",
    "- $BIC_k$ is the Bayes Information Criterion for a model with $k$ regressors (independent variables)\n",
    "- $n$ is the number of observations\n",
    "- $ln$ is the natural log, meaning a logarithm of base $e$, where $e$ is Euler's constant\n",
    "- $SS_{\\epsilon}$ is the sum of squared errors\n",
    "\n",
    "The lower the BIC, the more theoretically optimal the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c66856-b265-476b-b9b7-f2b61a78bd43",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abf4f1-95ed-4c5f-a3eb-5db087e690bb",
   "metadata": {},
   "source": [
    "Logistic regression is an extension of linear regression toward classification, and models the probabilities for classification problems. The model inherently solves binary classification problems, however extensions to integrate multiple classes exist.\n",
    "\n",
    "Linear regression does not work for classification, because it does not output probabilities, but rather a line or hyperplane that minimizes the distance between itself and the points. A linear model extrapolates and gives values lower than 0 and greater than $1$, which can not be treated as probabilities. \n",
    "\n",
    "Logistic regression uses the logistic function to squeeze the output of a linear equation to between $0$ and $1$.\n",
    "\n",
    "<h5>$logistic(x) = \\frac{1}{1+e^{-x}}$</h5>\n",
    "\n",
    "- $logistic(x)$ is the input $x$ transformed into the output of the function\n",
    "- $x$ is an instance of input\n",
    "- $e$ is Euler's constant, an irrational number starting with $2.7818...$ \n",
    "\n",
    "(**link to appendix**)\n",
    "\n",
    "The $x$ in the logistic function is recognizable as the linear regression model:\n",
    "\n",
    "$P(y^{(i)} = 1) = \\frac{1}{ 1 + exp(-(\\beta_0 + \\beta_1 x_1^{(i)} + \\ldots + \\beta_p x_p^{(i)}) ) }$\n",
    "\n",
    "- $P(y^{(i)} = 1)$ is the probability that an instance of $y$ equals the target class\n",
    "- $exp()$ is equivalent to saying Euler's constant $e$ exponentiated to the expression in brackets\n",
    "\n",
    "(**link to appendix**)\n",
    "\n",
    "The interpretation of weights in logistic regression differs from the interpretability of weights in linear regression, because the weighted sum is transformed into a probability. We can reformulate the equation for the interpretation so that only the linear term is on the right side of the formula.\n",
    "\n",
    "$log \\left( \\frac{P(y=1)}{P(y=0)} \\right) = \\beta_0 + \\beta_1 x_1^{(i)} + \\ldots + \\beta_p x_p^{(i)}$\n",
    "\n",
    "- $P(y^{(i)} = 1)$ is the probability that an instance of $y$ equals the target class\n",
    "- $P(y^{(i)} = 0)$ is the probability that an instance of $y$ equals the reference class\n",
    "- $log$ is the base-10 logarithm operation\n",
    "\n",
    "(**link to appendix**)\n",
    "\n",
    "We call the term in the brackets odds (the probability of one outcome divided by the probability of another), and wrapped in the logarithm, log-odds. A change in $x$ by one unit increases the log-odds ratio by the value of the corresponding weight, $\\beta_j$. Another, perhaps more intuitive way to interpret the weight $\\beta_j$ is that a change in $x$ by one unit increases the regular odds by $exp(\\beta_j)$. \n",
    "\n",
    "The components of the model can be interpreted as such:\n",
    "\n",
    "- Numerical Feature: if you increase the value of feature $x_j$ by one unit, the estimated odds change by a factor of $exp(\\beta_j)$.\n",
    "\n",
    "- Binary Categorical Feature: changing the feature $x_j$ from the reference category to the other category changes the estimated odds by a factor of $exp(\\beta_j)$.\n",
    "\n",
    "- Categorical Features: can be one-hot encoded (**link/appendix**) so that the binary categorical feature interpretation applies to each class.\n",
    "\n",
    "- Intercept $\\beta_0$: when all numerical features are zero and the categorical features are at the 'reference category' the estimated odds are $exp(\\beta_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763e620-d421-42f1-aa22-5534fb2e3dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb56eb7-157e-47e9-be3b-545061335515",
   "metadata": {},
   "source": [
    "#### Error Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb7a69-c25e-4f68-a945-32438eeb80d8",
   "metadata": {},
   "source": [
    "In a trial or experiment, an effect is either present or absent in each response, and there are four possible outcomes:\n",
    "\n",
    "1. A true positive a.k.a. a hit\n",
    "2. A false positive, a.k.a. a type I error or false alarm\n",
    "3. A true negative, a.k.a. a correct rejection\n",
    "4. A false negative, a.k.a. a type II error, or miss\n",
    "\n",
    "A confusion matrix represents these quantities visually.\n",
    "\n",
    "<img src=\"img/error_types.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=\"img/conf_matrix2.png\" style=\"height: 80px; width:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286deb6-a9e1-4198-8866-6629c127d086",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc699c4-80bf-4277-8a25-6a4d31174575",
   "metadata": {},
   "source": [
    "There are a number of metrics that can be calculated using the values from the confusion matrix, including accuracy:\n",
    "\n",
    "Accuracy is calculated as follows:\n",
    "\n",
    "$Accuracy = \\frac{ TP + TN }{ TP + FP + TN + FN }$\n",
    "\n",
    "The classification error rate is the inverse of classification accuracy:\n",
    "\n",
    "$Error Rate = \\frac{ FP + FN }{ TP + FP + TN + FN }$\n",
    "\n",
    "But used on a dataset with imbalanced classes, such as when 90% of observations belong to the same class, an unskilled model can provide 90% accuracy just by blindly picking the same class each time. To improve the accuracy further, you may need to use one of the other classification metrics, and in some scenarios, you will find one or more of the classification metrics to be more important than accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d4747-4673-4bd7-8d95-0da5499f96d6",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763db6fa-2da5-45f8-b6c7-4797fe93f9c6",
   "metadata": {},
   "source": [
    "Precision is the ratio of true positives to predicted positives. It is most used when there is a high cost for having false positives. Junk-mail classifiers should have a high degree of precision, so that they do not misclassify important emails as junk.\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f48b7-e768-41df-8f64-bed9c357cfb8",
   "metadata": {},
   "source": [
    "#### Sensitivity, a.k.a. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed863149-2b83-44f5-83d0-fa084b9eaefa",
   "metadata": {},
   "source": [
    "Sensitivity is important when concerned with identifying positive outcomes and the cost of a false positive is low. If predicting whether a patient has cancer, it is important that sensitivity be high so that we can capture as many positive cases as possible.\n",
    "\n",
    "$Sensitivity = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d85f5-ea16-4096-b63b-914f122d4d73",
   "metadata": {},
   "source": [
    "#### Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138c16a-f675-42ce-89fc-06006dcd92c3",
   "metadata": {},
   "source": [
    "Specificity is the ratio of true negatives to all negative outcomes. This is of interest if you are concerned about the accuracy of your negative rate and there is a high cost to a positive outcome. An example would be if you are an auditor looking over financial transactions and a positive outcome would mean a one-year investigation, but not finding one would cost very little.\n",
    "\n",
    "$Specificity = \\frac{TN}{TN+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90a8ca-482f-4396-97f3-aad7cc79bfd9",
   "metadata": {},
   "source": [
    "There are several other classification metrics, defined in the table below. \n",
    "\n",
    "**table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21356b-a2f2-4ed4-8e6b-013569f788d4",
   "metadata": {},
   "source": [
    "There are also measures that blend multiple classification metrics into one, such as the F-Measure, a.k.a. the F-Score or F1-Score:\n",
    "\n",
    "$\\text{F-Measure} = \\frac{ 2 ~\\times ~Precision ~\\times ~Recall }{ Precision ~\\times ~Recall }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebc50a-61c7-470e-a1c1-d762d16520e5",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b895d-3c96-4c8a-9238-64ae5174e07c",
   "metadata": {},
   "source": [
    "Models are sensitive to the data they are trained on - if not, it can be considered to under-fit the data. If the model is too sensitive, or the data is misrepresentative of the population, then the model can over-fit, producing high training accuracy but fail to generalize well toward new data. If you are comparing several models, such as a series of regression models fit with different combinations or transformations of features, then it is helpful to employ a validation strategy which holds back some of the data for a post-training evaluation of the model on this unseen validation or test set.\n",
    "\n",
    "This is called cross-validation, and with a little automation, we could repeat the process several times, each time shuffling the data and choosing a new holdout-set at random. This is called k-fold cross-validation and provides an even better idea of how well a model will generalize on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80444f-da9e-4695-9a43-15990229b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03521b4-4b4a-4d0b-b7f5-647daf247704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9409d-1750-40cc-9ae8-fe755a584e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bb9e0-2e1d-4224-999a-58abb35c99e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526ea66b-c27a-4cc8-b795-83eb1b5785d3",
   "metadata": {},
   "source": [
    "## ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabd46e-13ef-421d-bf1f-a5a77c5ad8ef",
   "metadata": {},
   "source": [
    "ANOVA quantifies the total amount of variability in a dataset, and determines how much of that is attributable to the independent (X) variables and how much is due to noise or unmeasured factors, and then computes the ratio of explained to unexplained variability. It is to determine the effects of categorical independent variables on a numerical dependent (y) variable. If the independent variable is naturally numerical rather than categorical, you can discretize it into a relatively small number of bins, or consider using regression instead.\n",
    "\n",
    "ANOVA creates a table of factors and levels. Factors are the independent variables in the study, and levels are the distinct categories or groups within each factor. Assumptions of ANOVA include:\n",
    "- Independence: observations are sampled randomly; one observation does not affect another\n",
    "- Normality: the population data is approximately normally distributed\n",
    "- Homogeneity of Variance: comparable levels of variance across all levels of the independent variables\n",
    "- Absence of Multicollinearity: absence of redundance (correlation) in the independent variables\n",
    "- No Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a303c-a933-417e-9e08-016a5f0edd56",
   "metadata": {},
   "source": [
    "#### One-Way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449ad69-8a30-4587-9022-b555e81ad6a7",
   "metadata": {},
   "source": [
    "An ANOVA with one factor and two levels is essentially equivalent to a t-test. Both tests are used to compare means between groups. In a t-test, you directly compare the means of two groups, while in ANOVA, you are assessing whether there is a significant difference in means among groups.\n",
    "\n",
    "$H_0: \\mu_1 = \\mu_2 = \\ldots$\n",
    "\n",
    "$H_A: \\mu_i \\neq \\mu_j$\n",
    "\n",
    "**discuss factors and levels**\n",
    "\n",
    "ANOVA relies on the sum of squares, which is very similar to variance.\n",
    "\n",
    "$$SS = \\sum_{i=1}^n (x_i - \\bar{x})^2$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2$$\n",
    "\n",
    "The only difference is that variance divides by $n-1$ (equivalent to factoring by $\\frac{1}{n-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40077f9b-fb63-4ca2-a2c9-7b1cc931ced9",
   "metadata": {},
   "source": [
    "#### Partitioning Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3f060-4021-44d1-a2eb-42aae61097fc",
   "metadata": {},
   "source": [
    "It is partitioned in three different ways:\n",
    "- $SS_{Total}$ compares every individual within every group to the global mean of the data \\bar{x}, computing the total variance.\n",
    "- $SS_{Between}$ looks at the mean within each level minus the global mean.\n",
    "- $SS_{Within}$ is also called the sum of squared errors, comparing the mean of each individual to the mean within the specific group.\n",
    "    \n",
    "<img src=\"img/SS_3.png\" style=\"height: 300px; width:auto;\">\n",
    "\n",
    "-  $x_{ij}$ is the observation indexed by individual $i$ in level $j$\n",
    "- $\\bar{x}$ is the global mean of the variable $x$\n",
    "- $\\bar{x_j}$ is the mean for the level indexed by $j$\n",
    "- $n_j$ is the number of individuals within the level indexed by $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548350cf-8561-4a53-ba6e-a726c55df944",
   "metadata": {
    "tags": []
   },
   "source": [
    "We then compute the mean of squares between, and the mean of squares within:\n",
    "    \n",
    "<h4>$\\text{MS}_{Between} = \\frac{\\text{SS}_{Between}}{\\text{df}_{Between}}$</h4>\n",
    "\n",
    "<h4>$\\text{MS}_{Within} = \\frac{\\text{SS}_{Within}}{\\text{df}_{Within}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a7ae3-471f-4ce0-aabb-eaeaedd9f678",
   "metadata": {},
   "source": [
    "#### The F-Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de913569-ebd2-4115-bbac-62ce2256f694",
   "metadata": {},
   "source": [
    "The ratio of variabilities (normalized to control for sample size) is called an F-statistic, and large F-values provide evidence against the null hypothesis $H_0$. The critical F-value for significance depends on the degrees of freedom.\n",
    "\n",
    "**one-way table**\n",
    "\n",
    "The F test statistic and F distribution are calculated as the ratio of two variances. It has two parameters, which are a degrees of freedom measure for the numerator and a degrees of freedom measure for the denominator. As with the normal distribution and t-distribution, critical values from the F-distribution are used to determine whether the observed F-statistic is significant or not.\n",
    "\n",
    "<h4>$F = \\frac{MS_{Between}}{MS_{Within}}$</h4>\n",
    "\n",
    "The p-value and F-ratio don't actually tell you which groups are different, they only tell you there is a difference somewhere. So it's necessary to do subsequent data visualization and t-tests to determine exactly which groups and levels. A p-value of less than the significance level indicates that at least one level is statistically significantly different from the mean of at least one other level.\n",
    "\n",
    "We can also evaluate the ANOVA by calculating an $R^2$ value, as:\n",
    "\n",
    "<h4>$R^2 = \\frac{SS_{Between}}{SS_{Total}}$</h4>\n",
    "\n",
    "And we can also calculate an adjusted R^2 (to control for number of parameters), as:\n",
    "\n",
    "<h4>$\\text{Adjusted } R^2 = 1 - \\frac{ (1-R^2)(n-1) }{ N - k - 1 }$</h4>\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $k$ is the number of parameters (variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5ff8e-6c5f-461b-b4cc-fdfe8c59a347",
   "metadata": {},
   "source": [
    "#### Two-Way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb73fc-b86d-4679-8e80-2e418473ab30",
   "metadata": {},
   "source": [
    "With two-way ANOVA, we are looking at the potential for interaction. If a medication has a different effect depending on whether you are young or old, that is an example of an interaction.\n",
    "\n",
    "The total variation is expressed as teh sum of variation across individuals within each group, plus the variation across different levels within each factor plus the variation of the interaction between the factors.\n",
    "\n",
    "<img src=\"img/SS_5.png\" style=\"height: 450px; width:auto;\">\n",
    "\n",
    "- $x_{ijk}$ is an observation of the $i^{th}$ observation of levels B, $j_{th}$ observation of level A, and $k^{th}$ observation of level B\n",
    "- $\\bar{x}_j$ is the average among the $j^{th}$ level of factor A\n",
    "- $\\bar{x}$ is the global average of variable $x$\n",
    "- $\\bar{x}_k$ is the avaerage among the $k^{th}$ level of factor B\n",
    "\n",
    "\n",
    "<img src=\"img/anova_two_way.png\" style=\"height: 250px; width:auto;\">\n",
    "\n",
    "- $SS_{Total}$ is the same concept of the sum of squares, and is really just the total variance of the dataset. \n",
    "\n",
    "- $SS_{Between}$ has two factors now because we have two factors in our design. A two-way ANOVA with three factors would have three SS_{Between}terms, and so on. For $SS_{Between}$, we're ignoring one of the factors, and computing the marginal variance. The multiplication by $bn$ in order to compute $SS_{A \\times B}$ is not big $N$, the number of observations in the sample, but rather by $n$, the number of observations in each level of factor $B$, multiplied by $b$, the number of levels in factor $B$. The reverse is true for factor $A$. So $bn$ and $an$ represent the total number of observations for the other factor.\n",
    "\n",
    "- $SS_{A \\times B}$ is the interaction between A and B. We take the individual cell mean and subtract the marginal mean from factor B and the marginal mean from factor A within each level, and then add this to the total mean across the entire dataset.\n",
    "\n",
    "If $p \\lt 0.05$, at least one level [for the group?] is significantly different from at least one other level. Determining which group requires data visualization and follow-up t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83433cc0-1924-448c-a79c-cdb9fb6754b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b09d1-844c-4415-8990-028e212c89aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7a944-c7c1-45fe-97c7-9cce179d90cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2afd0-05cf-407c-9e1f-a7a3d8cd28f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b23616-81fc-4e3c-8cca-0d8bacd5598a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
